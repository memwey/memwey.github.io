[{"content":"从一家创业公司离职, 去另外一家创业公司.\n在家待了一个月多一点, 期间解决了一下家里的大事小情, 复习刷题和面试, 开始巴西柔术的学习, 控制饮食和运动, 刷 Bilibili 什么的. 然而最重要的婚礼因为突如其来的疫情只能被迫延后了.\n不得不说, 广州的机会真的不多. 一开始找了几家中等级别的公司面试练手, 结果发现没有达到那种面的我怀疑人生然后努力复习的结果, 随手就面过, 回家接着摸鱼. 然后抱有比较大希望的腾讯和字节都因为各种各样奇怪的原因连面试都没有, 不得不说和某些公司就是没有缘分.\n离开的原因 其实我觉得还是离开上一家公司的太迟了. 年初拿到 Edison 的 Offer 的时候就应该离开了.\n有种观点认为, 对于一个创业来说, 最重要的是团队, 尤其是最初共同创业的团队, 业务都是在其次. 但是上一家公司在去年第三季度的时候就出现了一波离职潮, 基本上所有作为公司创始成员的老员工都离职了. 曾经工资都发不出来的时期没有离开, 现在拿到融资了业务有一些眉目了反而离开了, 可能这就是可以共患难而不能同富贵吧.\n当然可能并不是钱的问题. 老板自己承认自己有暴力沟通的问题. 不过, 能不能认识到是一回事, 能不能控制是另外一回事. 人总不可能不犯错, 往好了说是老板真性情, 往坏了说就是老板抓住一点错误就大做文章, 哭爹骂娘什么的.\n创业有一个相当不好平衡的问题. 创业公司当然希望团队成员能力超群, 能解决各种各样的问题, 但是除了少数带有光环的创业公司, 资金一般都不是那么充裕的. 对于优秀的求职者来说, 既然是创业, 风险更高, 为何要放弃差不多甚至更低的短期回报, 来承担更高的风险呢. 这也是上面提到的最初共同创业的团队非常重要的原因之一.\n在上一家公司, 老板可能是想出了解决办法. 团队中少数的优秀员工负责, 带领一些能力一般的员工. 作为后端负责人, 经常梳理产品设计, 团队代码审查就把工作时间占满了, 写代码只能加班. 有次私下跟老板吐槽一个产品经理, 根本发现不了用户需求, 一个功能上线了四五版, 都解决不了用户的问题. 老板一句话顶回来了: 你知道她一个月多少钱吗, 五千.\n在这种同事能力参差, 而老板有时又格外苛责的情况下, 压力确实比较大, 情绪比较消沉, 有段时间天天想请假, 想到要去上班就头疼. 最终还是决定离职了.\n选择的原因 又来到一家创业公司其实自己都蛮没想到的, 毕竟最初的构想是去大公司做一些基础架构的开发, 远离枯燥重复的业务开发.\n这家公司最吸引的可能就是微信顶级产品经理带队吧. 这几年的工作也算是接触过不少产品经理, 有厉害的也有水平一般的, 感觉产品经理是一个有手就能做的职业, 但是水平高的产品经理确实无法替代. 想近距离接触一下传说中的微信之母, 直接向张小龙汇报的产品经理, 看看她们的思维和做事是什么样的.\n另外, 这家公司光环比较强, 资金比较充足, 薪酬福利什么的都挺不错的, 完美的解决了创业公司的一些问题.\n不过个人还是觉得在目前公司主要产品上, 有一些隐患, 在熟人IM和陌生人社交上有点定位不清. 而且在熟人IM上, 很难看到短期内的盈利空间. 但是毕竟不是专业的产品经理, 所有的看法都是基于已有的产品, 比如微信, 陌陌, Soul 什么的. 厉害的产品经理可以发现别人发现不到的需求, 万一呢.\n","date":"2021-06-06T23:29:39+08:00","permalink":"https://memwey.github.io/p/%E4%B8%80%E4%B8%AA%E7%A6%BB%E8%81%8C%E5%B0%8F%E8%AE%B0/","title":"一个离职小记"},{"content":"既然都读了一些 Redis 的源码了, 可以继续再读一读, 今天整理一下之前简单了解过 Redis 持久化的内容.\n基础介绍 Redis 是一个很快的, 基于内存的数据库. 而在内存中则代表着易失性. Redis 提供了一些内置的功能, 让我们可以把数据保存到硬盘中. 对于 Redis 来说, 这些硬盘中的数据是一种 备份, 即在提供服务的过程中, Redis 只会通过某些方式构建并维护这些数据, 而不会读取这些数据. 这些数据当且仅当 Redis 进行数据恢复的时候使用.\n背景知识 首先, 说到持久化, 必须了解什么是 持久化. 即答: 当然是写到硬盘上就是持久化啦. 粗略的来说这样并没有错, 但是描述并不准确.\n可以先看一下 Antirez 做的总结:\n The first thing to consider is what we can expect from a database in terms of durability. In order to do so we can visualize what happens during a simple write operation:\n1: The client sends a write command to the database (data is in client\u0026rsquo;s memory).\n2: The database receives the write (data is in server\u0026rsquo;s memory).\n3: The database calls the system call that writes the data on disk (data is in the kernel\u0026rsquo;s buffer).\n4: The operating system transfers the write buffer to the disk controller (data is in the disk cache).\n5: The disk controller actually writes the data into a physical media (a magnetic disk, a Nand chip, \u0026hellip;)\n 这个流程描述了一个写操作的流程, 当然, 这个流程也是经过了相当的简化, 尤其是在步骤二和步骤三中, 为了提高效率, 操作系统内部会有复杂的, 多层次的缓存机制.\n如果只考虑数据库软件崩溃或者进程被终结, 而没有在操作系统内核上的丢失, 那么在完成步骤三之后, 就可以认为这个写操作是安全. 当操作被提交给了内核之后, 即使软件退出, 内核仍然会负责将数据传输到磁盘控制器.\n而当我们考虑到断电这样的情况时, 只有到达步骤五, 我们才能认为数据是安全的, 即数据存储在物理存储设备上.\n所以数据安全最重要的三, 四, 五阶段可以被总结为以下三个问题:\n 数据库软件使用系统调用将其用户空间缓冲区转移到内核缓冲区的频率是多少 - 对应第三阶段 内核什么时候会将缓冲区刷新 (flush) 到磁盘控制器 - 对应第四阶段 磁盘控制器什么时候向物理介质写入一次数据 - 对应第五阶段  RDB RDB (Redis Databse) 持久化按指定的时间间隔执行数据集的时间点快照\nAOF AOF (Append Only File) 持久化记录服务器接收到的每个写操作, 这些操作将在服务器启动时重放, 重建之前的数据集. 命令的日志格式与 Redis 协议本身相同, 不过带有附加格式. 当日志变得太大时, Redis 可以在后台重写它.\n参考资料  Redis Persistence - Redis Redis persistence demystified  ","date":"2021-04-22T21:29:08+08:00","permalink":"https://memwey.github.io/p/redis-persistence/","title":"Redis Persistence"},{"content":"结构推测 今天与人争执的时候觉得应该探究一下 Redis 中 ZINTERSTORE 的实现. 先看文档中的描述\n ZINTERSTORE destination numkeys key [key \u0026hellip;] [WEIGHTS weight [weight \u0026hellip;]] [AGGREGATE SUM|MIN|MAX]\n  Available since 2.0.0.\nTime complexity: O(NK)+O(Mlog(M)) worst case with N being the smallest input sorted set, K being the number of input sorted sets and M being the number of elements in the resulting sorted set.\n 大致意思就是最坏情况下时间复杂度是 O(N*K)+O(M*log(M)), 其中 N 是输入中元素最少的 sorted set 的元素数目, K 是输入的 sorted set 的数量, M 是结果中的 sorted set 中元素的数目.\n其实光从时间复杂度就能推测出来大致的操作了. O(M*log(M)) 这个复杂度很有可能是一次排序操作, 而且和结果中的元素数目相关, 那么很有可能是先取交集之后得出 M 个元素, 再在 M 个元素中进行排序的. 当然, 也有可能是相应的, 在有序的序列中进行插入的操作.\n而 N 是最小的 sorted set 的元素个数. 这个一开始我有点想不明白, 难道不应该是最大的才对吗, 因为这里我还是想着去遍历其他的 sorted set 进行比较. 但是思考一下, sorted set 也是 set 嘛, 完全可以使用 O(1) 的效率在其中进行查找. 这样实际就变成了分别在 (K - 1) 个 sorted set 中寻找 N 个元素, 这样自然就是 O(N*K) 了.\n源码分析 以下源码基于 Redis 3.0 分析, 实际的函数操作为\n1 2  /* https://github.com/redis/redis/blob/3.0/src/t_zset.c#L1905 */ void zunionInterGenericCommand(redisClient *c, robj *dstkey, int op)   这里我们可以看到, 在 Redis 的源码中将 ZINTERSTORE 和 ZUNIONSTORE 放在了一起处理, 使用 op 做区分.\n随后是一段漫长的代码, 主要是处理输入参数的, 比如要操作的 sorted set 的列表, 存到了 src 中; 然后处理 WEIGHTS 和 AGGREGATE. 代码比较长而且逻辑也很简单.\n1 2 3 4 5  /* https://github.com/redis/redis/blob/3.0/src/t_zset.c#L1993 */ /* sort sets from the smallest to largest, this will improve our * algorithm\u0026#39;s performance */ qsort(src,setnum,sizeof(zsetopsrc),zuiCompareByCardinality);   然后这里把所有 sorted set 按元素的个数从小到大排列, 以提高效率. 随后是具体的取交集的代码.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45  /* https://github.com/redis/redis/blob/3.0/src/t_zset.c#L2001 */ if (op == REDIS_OP_INTER) { /* Skip everything if the smallest input is empty. */ if (zuiLength(\u0026amp;src[0]) \u0026gt; 0) { /* Precondition: as src[0] is non-empty and the inputs are ordered * by size, all src[i \u0026gt; 0] are non-empty too. */ zuiInitIterator(\u0026amp;src[0]); while (zuiNext(\u0026amp;src[0],\u0026amp;zval)) { double score, value; score = src[0].weight * zval.score; if (isnan(score)) score = 0; for (j = 1; j \u0026lt; setnum; j++) { /* It is not safe to access the zset we are * iterating, so explicitly check for equal object. */ if (src[j].subject == src[0].subject) { value = zval.score*src[j].weight; zunionInterAggregate(\u0026amp;score,value,aggregate); } else if (zuiFind(\u0026amp;src[j],\u0026amp;zval,\u0026amp;value)) { value *= src[j].weight; zunionInterAggregate(\u0026amp;score,value,aggregate); } else { break; } } /* Only continue when present in every input. */ if (j == setnum) { tmp = zuiObjectFromValue(\u0026amp;zval); znode = zslInsert(dstzset-\u0026gt;zsl,score,tmp); incrRefCount(tmp); /* added to skiplist */ dictAdd(dstzset-\u0026gt;dict,tmp,\u0026amp;znode-\u0026gt;score); incrRefCount(tmp); /* added to dictionary */ if (sdsEncodedObject(tmp)) { if (sdslen(tmp-\u0026gt;ptr) \u0026gt; maxelelen) maxelelen = sdslen(tmp-\u0026gt;ptr); } } } zuiClearIterator(\u0026amp;src[0]); } }   当然, 如果按元素数从小到大排序的第一个 sorted set 元素数为 0, 那就可以直接返回了, 随后在此 sorted set 上建立一个 Iterator, 主要就是帮助在 sorted set 上做遍历的, 因为在 sorted set 中, 遍历不是一个常用操作, 排序才是.\n然后就是通过 while (zuiNext(\u0026amp;src[0],\u0026amp;zval)) 取出第一个 sorted set 中的每一个元素, 再用 for (j = 1; j \u0026lt; setnum; j++) 将其在每一个其他的 sorted set 中查找一遍.\n当两个 sorted set 指向同一个对象是, 那么毫无疑问一定会有同样的元素. 否则就在 src[j] 中查找当前元素. zuiFind(\u0026amp;src[j],\u0026amp;zval,\u0026amp;value) 实现了这个操作. 具体的代码在后面分析. 如果找不到的话, 则没有必要继续找下去了, 跳出即可.\n查找的过程中也使用了 zunionInterAggregate(\u0026amp;score,value,aggregate) 来更新当前元素的 score, 具体的 score 更新规则是根据 AGGREGATE 参数来定的.\n当元素在所有的 sorted set 中时, 就可以把这个元素添加进结果的 sorted set 中了. 这里就是根据 zval 和 score 往 dstzset 里面插入元素. 因为 sorted set 里面既有 dict 也有 skiplist, 所以两个都要添加.\n这里还有一个操作就是更新最大元素的长度, 这个和 sorted set 的内部优化有关.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  /* https://github.com/redis/redis/blob/3.0/src/t_zset.c#L2119 */ if (dbDelete(c-\u0026gt;db,dstkey)) { signalModifiedKey(c-\u0026gt;db,dstkey); touched = 1; server.dirty++; } if (dstzset-\u0026gt;zsl-\u0026gt;length) { /* Convert to ziplist when in limits. */ if (dstzset-\u0026gt;zsl-\u0026gt;length \u0026lt;= server.zset_max_ziplist_entries \u0026amp;\u0026amp; maxelelen \u0026lt;= server.zset_max_ziplist_value) zsetConvert(dstobj,REDIS_ENCODING_ZIPLIST); dbAdd(c-\u0026gt;db,dstkey,dstobj); addReplyLongLong(c,zsetLength(dstobj)); if (!touched) signalModifiedKey(c-\u0026gt;db,dstkey); notifyKeyspaceEvent(REDIS_NOTIFY_ZSET, (op == REDIS_OP_UNION) ? \u0026#34;zunionstore\u0026#34; : \u0026#34;zinterstore\u0026#34;, dstkey,c-\u0026gt;db-\u0026gt;id); server.dirty++; } else { decrRefCount(dstobj); addReply(c,shared.czero); if (touched) notifyKeyspaceEvent(REDIS_NOTIFY_GENERIC,\u0026#34;del\u0026#34;,dstkey,c-\u0026gt;db-\u0026gt;id); } zfree(src);   随后就是收尾工作. 如果目标的坑上已经有值了, 就毫不犹豫的干掉它. 然后再看作为结果的 sorted set. 如果它满足转化为 ziplist 的条件, 就可以把它转化为 ziplist. 后面是一些 hook 的通知, 可以暂时忽略. 如果结果为空, Integer reply 会返回 0, 否则会返回结果中元素的个数.\n至此大致流程已经结束了. 如同推测的那样, 算法复杂度完美的反映了操作的内部过程和数据结构. 不过, 我们还有几个问题没有解决. zuiFind 中的具体操作是什么, ziplist 又是什么.\n搁这搁这 这个标题充分提现了递归的思想. 学习新东西, 然后发现另一些新东西, 然后再学习这些新东西, 然后\u0026hellip;\u0026hellip;\n首先来看 zuiFind\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  /* https://github.com/redis/redis/blob/3.0/src/t_zset.c#L1826 */ int zuiFind(zsetopsrc *op, zsetopval *val, double *score) { if (op-\u0026gt;subject == NULL) return 0; if (op-\u0026gt;type == REDIS_SET) { if (op-\u0026gt;encoding == REDIS_ENCODING_INTSET) { if (zuiLongLongFromValue(val) \u0026amp;\u0026amp; intsetFind(op-\u0026gt;subject-\u0026gt;ptr,val-\u0026gt;ell)) { *score = 1.0; return 1; } else { return 0; } } else if (op-\u0026gt;encoding == REDIS_ENCODING_HT) { dict *ht = op-\u0026gt;subject-\u0026gt;ptr; zuiObjectFromValue(val); if (dictFind(ht,val-\u0026gt;ele) != NULL) { *score = 1.0; return 1; } else { return 0; } } else { redisPanic(\u0026#34;Unknown set encoding\u0026#34;); } } else if (op-\u0026gt;type == REDIS_ZSET) { zuiObjectFromValue(val); if (op-\u0026gt;encoding == REDIS_ENCODING_ZIPLIST) { if (zzlFind(op-\u0026gt;subject-\u0026gt;ptr,val-\u0026gt;ele,score) != NULL) { /* Score is already set by zzlFind. */ return 1; } else { return 0; } } else if (op-\u0026gt;encoding == REDIS_ENCODING_SKIPLIST) { zset *zs = op-\u0026gt;subject-\u0026gt;ptr; dictEntry *de; if ((de = dictFind(zs-\u0026gt;dict,val-\u0026gt;ele)) != NULL) { *score = *(double*)dictGetVal(de); return 1; } else { return 0; } } else { redisPanic(\u0026#34;Unknown sorted set encoding\u0026#34;); } } else { redisPanic(\u0026#34;Unsupported type\u0026#34;); } }   先忽略掉 set 的操作, 来看 sorted set. 我们又见到了收尾工作时出现过的 ziplist.\n The ziplist is a specially encoded dually linked list that is designed to be very memory efficient. It stores both strings and integer values, where integers are encoded as actual integers instead of a series of characters. It allows push and pop operations on either side of the list in O(1) time. However, because every operation requires a reallocation of the memory used by the ziplist, the actual complexity is related to the amount of memory used by the ziplist.\n 查了一下资料, 这是 Redis 在面对小元素时可以做的一个内存优化, 本体是一个经过特殊编码的双向链表. 经过特殊编码后的数据会变得更加紧凑, 连续的内存使用也对于缓存更加友好. 有两个配置决定了 sorted set 中使用 ziplist 的阈值.\n1 2  #define REDIS_ZSET_MAX_ZIPLIST_ENTRIES 128 #define REDIS_ZSET_MAX_ZIPLIST_VALUE 64   图. 有序集合\n然后我们尴尬的发现, 在 ziplist 查找一个元素实际上是一个遍历, 时间复杂度为 O(N), 如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  /* https://github.com/redis/redis/blob/3.0/src/t_zset.c#L915 */ unsigned char *zzlFind(unsigned char *zl, robj *ele, double *score) { unsigned char *eptr = ziplistIndex(zl,0), *sptr; ele = getDecodedObject(ele); while (eptr != NULL) { sptr = ziplistNext(zl,eptr); redisAssertWithInfo(NULL,ele,sptr != NULL); if (ziplistCompare(eptr,ele-\u0026gt;ptr,sdslen(ele-\u0026gt;ptr))) { /* Matching element, pull out score. */ if (score != NULL) *score = zzlGetScore(sptr); decrRefCount(ele); return eptr; } /* Move to next element. */ eptr = ziplistNext(zl,sptr); } decrRefCount(ele); return NULL; }   不过当 zset 使用 REDIS_ENCODING_SKIPLIST 作为 encoding 的时候, 使用 Hash table 做查询的时间复杂度是 O(1) 这是肯定的.\n当然, 这里面其实还有很多细节没有说到, 比如 ziplist 的内部表示, 和 zset 混在一起的 set, zset 中的 dict 和 skiplist 的详细分析等等. 下次有机会的吧.\n参考资料  The ziplist representation 压缩列表 — Redis 设计与实现  ","date":"2021-04-21T20:30:43+08:00","permalink":"https://memwey.github.io/p/redis-zinterstore/","title":"Redis ZINTERSTORE"},{"content":"随着工作经验的积累, 越来越感到仅仅写出 Elegent Code 是不够的, 特别是当多人合作开发的时候, 遇到种种结构混乱, 代码冗余, 模块耦合, 依赖耦合的问题. 这个时候, 就需要更高的要求, Elegent Project, 自顶向下的改善项目的结构.\n仅仅实现业务逻辑是很容易的, 而大量的时间会用在测试, Debug 上.\n以下面一段代码为例\n参考链接  Standard Go Project Layout   ","date":"2021-02-19T18:10:22+08:00","permalink":"https://memwey.github.io/p/write-elegent-project-with-golang/","title":"Write Elegent Project With Golang"},{"content":"基础知识 LRU (Least recently used) 是一个非常常用的缓存置换算法.\n在缓存空间有限的情况下, 在新的数据写入时, 需要淘汰一些旧数据. 一般期望最不可能被继续访问的数据淘汰. 由于无法对未来的情况进行预测, 只能基于现有的信息推测.\nLRU 基于这样一个假定, 在一个时间点上, 如果一个数据距离上次被访问的时间越长, 则这个数据在未来被访问的可能性越小. 所以, 应该淘汰掉距离上次被访问的时间最久的数据.\n常见的 LRU 实现是使用一个双向链表, 当 Item 被访问时, 将其移动到链表的头部. 当缓存不足时, 从链表的尾部开始淘汰.\n单纯的双向链表的实现实际上是不太符合现实的. 一般的, 我们希望缓存可以尽快被检索到, 而在双向链表中检索 Item 的效率是 O(n), 特别是检索的 Item 在链表中不存在时, 效率稳定的是 O(n). 这样, 在缓存系统中维护这个双向链表的成本是非常高的. 一般的, 会将 哈希表 或者 二叉搜索树 和双向链表组合起来, 在更高效率的数据结构中记录 Key, 并在 Key 中记录 Item 在双向链表中的指针.\n另外的, 在并发量较大的时候, 双向链表中的操作需要加锁, 否则链表很容易出问题.\n具体实现 Redis 2.8 在较早版本的 Redis 上, 并没有实现 LRU. 在后续 2.8 添加的时候, 并没有使用常见的双向链表的方式来实现 LRU, 而使用了一个近似的实现.\n从空间和时间上考虑, Redis 中的 Key 的数量可能非常的多, 双向链表可能会非常大, 占用内存非常多; 另一方面, Redis 中的操作可能也非常频繁, 每一次访问都需要操作一次双向链表, 在时间上也显得非常不划算.\nAntirez 在 Redis Object 中挤出了 24 个位元 bits, 并用其存储按秒计算的 unix timestamp 的低 24 位. 这个被称为 LRU clock.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  /* https://github.com/redis/redis/blob/3.0/src/redis.h#L420 */ /* A redis object, that is a type able to hold a string / list / set */ /* The actual Redis Object */ #define REDIS_LRU_BITS 24 #define REDIS_LRU_CLOCK_MAX ((1\u0026lt;\u0026lt;REDIS_LRU_BITS)-1) /* Max value of obj-\u0026gt;lru */#define REDIS_LRU_CLOCK_RESOLUTION 1000 /* LRU clock resolution in ms */typedef struct redisObject { unsigned type:4; unsigned encoding:4; unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */ int refcount; void *ptr; } robj;   24 个 bits 明显不够存储一个完整的时间戳, 当第二十四位向前进位的时候, 就会发生溢出. 此时, 最近被访问的 Item 的 LRU clock 反而较小, 更容易被淘汰. 考虑到这个溢出需要 194 天, 而 Redis 中的操作应该比较频繁, 所以 antirez 认为这个问题可以接受.\n理论上, 可以精心构造一些数据, 让 Redis 的 LRU 失效. 比如, 总是在溢出前访问一个 Item, 这个 Item 的 LRU clock 总是很大, 虽然这个 Item 的访问周期总是 194 天才访问一次, 但是它总不会被淘汰.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  /* https://github.com/redis/redis/blob/3.0/src/db.c#L43 */ robj *lookupKey(redisDb *db, robj *key) { dictEntry *de = dictFind(db-\u0026gt;dict,key-\u0026gt;ptr); if (de) { robj *val = dictGetVal(de); /* Update the access time for the ageing algorithm. * Don\u0026#39;t do it if we have a saving child, as this will trigger * a copy on write madness. */ if (server.rdb_child_pid == -1 \u0026amp;\u0026amp; server.aof_child_pid == -1) val-\u0026gt;lru = LRU_CLOCK(); return val; } else { return NULL; } }   接下来的问题在于如何找到最久没有被访问的 Item. 如果一定要找到最久没有被访问到的 Item, 那么需要遍历所有的 Key, 而且在遍历的过程中, 要么禁止在这期间做任何的访问操作, 要么可能出现找到的 Key 恰好又刚刚被访问到的问题.\nAntirez 在这里又使用了一个近似的实现, 随机选取 3 个 Key, 把他们之中最久没有被访问到的淘汰. 随后, 这个数值变成了可配置项 maxmemory-samples , 默认值是 5. 考虑到选出的结果不一定是最好的, 但是很大可能不是一个坏的结果, 即选出一个非常近被访问的 Item, 这个实现还算可以接受.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  /* https://github.com/redis/redis/blob/2.8/src/redis.c#L2982 */ /* volatile-lru and allkeys-lru policy */ else if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_LRU || server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) { for (k = 0; k \u0026lt; server.maxmemory_samples; k++) { sds thiskey; long thisval; robj *o; de = dictGetRandomKey(dict); thiskey = dictGetKey(de); /* When policy is volatile-lru we need an additional lookup * to locate the real key, as dict is set to db-\u0026gt;expires. */ if (server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) de = dictFind(db-\u0026gt;dict, thiskey); o = dictGetVal(de); thisval = estimateObjectIdleTime(o); /* Higher idle time is better candidate for deletion */ if (bestkey == NULL || thisval \u0026gt; bestval) { bestkey = thiskey; bestval = thisval; } } }   Redis 3.0 Antirez 在 3.0 版本中进一步提升了近似算法的准确性. 一个显而易见的方法是, 通过过去累计的信息来提升准确性.\nRedis 中维护了一个默认大小为 16 的 pool, 里面存储了备选的 Key. 当需要淘汰时, 从随机选择的 N 个 Key 中与 pool 中的 Key 做对比, 在 pool 中维护其中最久没有被访问到的 16 个 Key, 然后在 pool 中淘汰其中最久没有被访问到的 Item. 这个 pool 非常类似于小顶堆.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  /* https://github.com/redis/redis/blob/3.0/src/redis.c#L3275 */ /* volatile-lru and allkeys-lru policy */ else if (server.maxmemory_policy == REDIS_MAXMEMORY_ALLKEYS_LRU || server.maxmemory_policy == REDIS_MAXMEMORY_VOLATILE_LRU) { struct evictionPoolEntry *pool = db-\u0026gt;eviction_pool; while(bestkey == NULL) { evictionPoolPopulate(dict, db-\u0026gt;dict, db-\u0026gt;eviction_pool); /* Go backward from best to worst element to evict. */ for (k = REDIS_EVICTION_POOL_SIZE-1; k \u0026gt;= 0; k--) { if (pool[k].key == NULL) continue; de = dictFind(dict,pool[k].key); /* Remove the entry from the pool. */ sdsfree(pool[k].key); /* Shift all elements on its right to left. */ memmove(pool+k,pool+k+1, sizeof(pool[0])*(REDIS_EVICTION_POOL_SIZE-k-1)); /* Clear the element on the right which is empty * since we shifted one position to the left. */ pool[REDIS_EVICTION_POOL_SIZE-1].key = NULL; pool[REDIS_EVICTION_POOL_SIZE-1].idle = 0; /* If the key exists, is our pick. Otherwise it is * a ghost and we need to try the next element. */ if (de) { bestkey = dictGetKey(de); break; } else { /* Ghost... */ continue; } } } }   pool 中排序操作的核心代码在 evictionPoolPopulate 函数中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  /* https://github.com/redis/redis/blob/3.0/src/redis.c#L3145 */ #define EVICTION_SAMPLES_ARRAY_SIZE 16 void evictionPoolPopulate(dict *sampledict, dict *keydict, struct evictionPoolEntry *pool) { int j, k, count; dictEntry *_samples[EVICTION_SAMPLES_ARRAY_SIZE]; dictEntry **samples; /* Try to use a static buffer: this function is a big hit... * Note: it was actually measured that this helps. */ if (server.maxmemory_samples \u0026lt;= EVICTION_SAMPLES_ARRAY_SIZE) { samples = _samples; } else { samples = zmalloc(sizeof(samples[0])*server.maxmemory_samples); } count = dictGetSomeKeys(sampledict,samples,server.maxmemory_samples); for (j = 0; j \u0026lt; count; j++) { unsigned long long idle; sds key; robj *o; dictEntry *de; de = samples[j]; key = dictGetKey(de); /* If the dictionary we are sampling from is not the main * dictionary (but the expires one) we need to lookup the key * again in the key dictionary to obtain the value object. */ if (sampledict != keydict) de = dictFind(keydict, key); o = dictGetVal(de); idle = estimateObjectIdleTime(o); /* Insert the element inside the pool. * First, find the first empty bucket or the first populated * bucket that has an idle time smaller than our idle time. */ k = 0; while (k \u0026lt; REDIS_EVICTION_POOL_SIZE \u0026amp;\u0026amp; pool[k].key \u0026amp;\u0026amp; pool[k].idle \u0026lt; idle) k++; if (k == 0 \u0026amp;\u0026amp; pool[REDIS_EVICTION_POOL_SIZE-1].key != NULL) { /* Can\u0026#39;t insert if the element is \u0026lt; the worst element we have * and there are no empty buckets. */ continue; } else if (k \u0026lt; REDIS_EVICTION_POOL_SIZE \u0026amp;\u0026amp; pool[k].key == NULL) { /* Inserting into empty position. No setup needed before insert. */ } else { /* Inserting in the middle. Now k points to the first element * greater than the element to insert. */ if (pool[REDIS_EVICTION_POOL_SIZE-1].key == NULL) { /* Free space on the right? Insert at k shifting * all the elements from k to end to the right. */ memmove(pool+k+1,pool+k, sizeof(pool[0])*(REDIS_EVICTION_POOL_SIZE-k-1)); } else { /* No free space on right? Insert at k-1 */ k--; /* Shift all elements on the left of k (included) to the * left, so we discard the element with smaller idle time. */ sdsfree(pool[0].key); memmove(pool,pool+1,sizeof(pool[0])*k); } } pool[k].key = sdsdup(key); pool[k].idle = idle; } if (samples != _samples) zfree(samples); }   参考资料  Random notes on improving the Redis LRU algorithm Using Redis as an LRU cache  ","date":"2020-12-25T14:26:24+08:00","permalink":"https://memwey.github.io/p/redis-lru/","title":"Redis LRU"},{"content":"原本的台式机, E3-1231v3 搭配上在挖矿狂潮前买的 GTX 1070, 在这个普遍挤牙膏的时期感觉再战三年不成问题. 不过既然有了自己的书房, 书房里没有电脑是万万不能的. 看了一下自己的钱包和京东的无货, 决定用 新平台 + 老显卡, 老平台 + 入门显卡 这样的组合, 让一台电脑变成两台.\n挑选过程 新平台暂且不表, 有空另外记录. 这段其实跳过也行, 都是一些流水账.\n首先还是决定给老平台增加一些便携性, 毕竟性能够用就好, 租房住的生活还是要考虑搬家的, 之前的全铝加钢化玻璃机箱搬起来确实不方便. 另外, 对之前的乞丐版主板和电源也都不太满意, 但是问题最大的就是主板, LGA 1150 平台的 ITX 主板数量稀少且价格昂贵. 转换思路想一想, 机箱, 主板, 电源, 这不就是一个准系统吗.\n淘宝逛了一下发现, 主要是 DELL 和 HP 的 SFF (Small Form Factor) 型准系统比较符合需求. 主要型号包括 DELL OptiPlex 3020, DELL OptiPlex 7020, DELL OptiPlex 9020, DELL Precision T1700, HP ProDesk 400 G1, HP ProDesk 600 G1, HP ProDesk 800 G1, HP Z230. 同系列的产品很好比较, 数字越大约好, 不过其实也没太大区别. HP 的机器相比 DELL 的稍微大一些, 扩展性稍强.\nDELL Precision T1700 在以上型号中脱颖而出的原因可能就是因为它是 Workstation 和 C226 主板芯片组和它不属于搞的人头晕的家族式型号吧.\n系统介绍  尺寸: 290.00mm * 92.60mm * 312.00mm 体积: 8.38L 重量: 5.30KG 电源输出: 255W  图. 机箱前面板\n体积完全可以称得上小巧. C226 主板可以让 LGA 1150 的 E3-1231v3 和 DDR3 的两条内存条发挥余热和体验工作站主板. 支持一张半高显卡可以在 E3-1231v3 没有核心显卡和一定的游戏需求中找到一个能用的解决方案.\n硬件配置  CPU: Intel Xeon E3-1231 v3 @ 3.40GHz 4C8T RAM: G.SKILL 8G 1600MHz (x2) GPU: NVIDIA GeForce GTX 1650 4G SSD: GLOWAY STK240GS3-S7 Wi-Fi: Intel Dual Band Wireless-AC 7265  其余诸如主板, 机箱, 电源, 散热器都是准系统自带的.\n由于显卡必须要 单槽位 的 半高 显卡, 所以这块 翔升 GTX1650 4GD5 战刀 应该是目前(2020-11-01)能买到的现在已经买不到了, 刚看了一下最强的显卡了.\n图. 但是长的有点丑\n其余配件就都是从老机器上拆下来的了, 唯一有点例外的是无线网卡, 因为是 PCIe 网卡, 之前是全高挡板, 淘宝算上邮费 7 块钱买了个半高挡板解决. 从北京快递过来邮费只要 4 块钱, 我也是惊了.\n装机体验 图. 机箱后面板\n小小机身拥有前置 4 个 USB 加后置 6 个 USB, 数量惊人. 甚至还带了 PS/2 键鼠接口和 RS-232 串行接口. 显示输出带了两个 DP 和一个 VGA 这对新老搭档, 缺少中青代的 HDMI 和 DVI 可能会有点不便. 用独立显卡就无所谓了.\n机箱完全是免螺丝拆装的, 维护性很好. 机箱内部有大量塑料的部分, 主要起调整风道和固定的作用. 电源和前面板风扇从前面板下方吸风, 从机箱后部排出热风.\n图. 机箱内部\n虽然主板上有三个 SATA 接口, 但实际上只有一个标准的硬盘供电, 凑合着和一个非标准的光驱供电一起从主板上引出, 所以想要使用超过一个硬盘要想想办法, 可以使用一转二的硬盘供电什么的.\n如果使用单个固态硬盘的话, 直接把固态硬盘塞到那个 3.5 寸机械硬盘位就好了, 不过有机械硬盘的话, 固态硬盘就不太好安放了, 好在空间还算多, 固态硬盘也很坚强, 找个地方塞一下问题不大.\n图. 凌乱的内部\n非标准的电源只有 255W, 而且只有一个 8pin 的主板供电和一个 4pin 的 CPU 供电, 没有任何 PCIe 供电. 不过反正半高显卡也基本都是能被 PCIe 16X 的 75W 供电满足的.\n四个标准台式机内存插槽够用, 给已经成为时代的眼泪的 DDR3 内存找一个家. 插槽上有清晰的编号, 按照顺序插上即可.\n主板提供了两个 PCIe 插槽, 一个全长 x16, 另一个是 x4. 但是问题是 16x 的反而 4x 的下面, 而且距离再下面的电源很近, 所以要想让显卡充分发挥只能用单槽位的. 不过这块 翔升 GTX1650 4GD5 战刀 是涡轮散热, 从显卡顶端的风扇吸风到尾部排出, 和机箱原有的前面板风扇风道一致, 散热应该还好. 有机友反馈显卡太长会挡住部分 SATA 接口, 这块显卡因为更短了, 所以没有这个情况.\n主板上真的是一个多余的接口都没有, 导致 PCIe 无线网卡蓝牙功能所需的 USB 插头无处安放. 好吧, 小问题.\n机箱还有一个内置的小音响, 虽然音质什么的只能说是听个响, 甚至还不怎么响, 但是应急什么的还是挺好用的.\n原配的散热器是全铝的, 之前甚至都没见过这么寒酸的散热器. 在万能的淘宝上发现了可用的铜芯散热器 J9G15, 据商家所说是原配给更高端的 XE2 机型的散热器, 比全铝的重了有 90 克. 花费 46.8 入手一只换上, 感觉温度确实下降了不少.\n图. 机箱大小参考\n参考资料  Dell Precision T1700 小型计算机 用户手册 Dell Precision T1700 SFF 攒机与使用  年轻人的第一台Precision  ","date":"2020-12-04T00:52:03+08:00","permalink":"https://memwey.github.io/p/dell-precision-t1700-sff/","title":"DELL Precision T1700 SFF"},{"content":"https://github.com/lando/lando/issues/1294\nhttps://github.com/docker/for-linux/issues/955\n","date":"2020-06-19T16:36:59+08:00","permalink":"https://memwey.github.io/p/moby-with-fedora-32/","title":"Moby With Fedora 32"},{"content":"https://imroc.io/posts/kubernetes/let-ingress-enable-free-https-with-cert-manager/\nhttps://cert-manager.io/docs/tutorials/acme/ingress/#step-5-deploy-cert-manager\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-an-nginx-ingress-on-digitalocean-kubernetes-using-helm\n","date":"2019-11-06T10:29:17+08:00","permalink":"https://memwey.github.io/p/k8s-cert/","title":"K8s Cert"},{"content":"Kubernetes 旨在在一组机器上运行分布式系统。分布式系统的本质使网络成为 Kubernetes 部署中的核心且必要的部分，了解 Kubernetes 网络模型将使你能够正确地运行，监控和排查在 Kubernetes 上运行的应用程序。\n网络是一个拥有许多成熟技术的广阔领域。对于不那么熟悉这个领域的人来说，这可能会令人感到不适，因为大多数人已经对网络有了先入为主的观念，而在 Kubernetes 中有很多新旧概念需要理解并将它们融合为一个整体。一个粗略的列表可能包括诸如网络命名空间，虚拟接口，IP 转发和网络地址转换之类的技术。本指南旨在通过讨论各种 Kubernetes 依靠的技术以及对这些技术是如何用来实现 Kubernetes 网络模型的描述来讲解 Kubernetes 网络模型。\n本指南相当长，分为几个部分。我们首先讨论一些基本的 Kubernetes 术语以确保在整个指南中正确使用术语，然后讨论 Kubernetes 的网络模型以及它规定的设计和实现决策。接下来是本指南中最长且最有趣的部分：通过几种不同的用例深入讨论流量是如何在 Kubernetes 中被路由的。\n如果您对一些网络术语，本指南附有网络术语词汇表。\n目录  1 Kubernetes 基础知识 2 Kubernetes 网络模型 3 Container 到 Container 网络 4 Pod 到 Pod 网络 5 Pod 到 Service 网络 6 Internet 到 Service 网络 6.1 Egress 6.2 Ingress 7 总结 8 术语表  1 Kubernetes 基础知识 Kubernetes 是由一些核心概念构建而成的，这些核心概念被组合成越来越强大的功能。本节列出了这些概念，并提供了简要概述以帮助促进讨论。Kubernetes 的功能远不止这里列出的内容，本节作为入门使读者可以在后续部分中参考。如果您已经熟悉 Kubernetes，请随意跳过本节。\n1.1 Kubernetes API 服务器 在 Kubernetes 中，一切都是由 Kubernetes API 服务器 (kube-apiserver) 提供的 API 调用。API 服务器是通往维护着应用程序集群的所需状态的 etcd 数据存储的网关。要更新 Kubernetes 集群的状态，您需要对 API 服务器进行 API 调用以描述所需的状态。\n1.2 Controllers Controller 是用于构建 Kubernetes 的核心抽象概念之一。在使用 API 服务器声明集群的期望状态后，Controller 将通过持续监视 API 服务器的状态并对所以变化做出反应来确保集群的当前状态与期望状态相匹配。Controller 使用一个简单的循环进行操作，该循环不断地根据群集当前状态对比群集的期望状态。如果存在任何差异，则控制器执行任务以确保当前状态与期望状态符合。用伪代码来表示：\n1 2 3 4 5 6 7 8  while true: X = currentState() Y = desiredState() if X == Y: return # Do nothing else: do(tasks to get to Y)   例如，当您使用 API 服务器创建新的 Pod 时，kube-scheduler (一个 Controller) 会注意到更改，并决定 Pod 部署在群集中的位置。然后，它使用 API 服务器 (由etcd支持) 写入状态变更。然后，kubelet (另一个 Controller) 会注意到新的更改，并建立所需的网络功能以使 Pod 在群集中可达。在这里，两个不同的控制器对两个不同的状态更改做出反应，以使集群的实际状况与用户的意图相匹配。\n1.3 Pods Pod 相当于 Kubernetes 中的原子 - 用于构建应用程序的最小可部署对象。单个 Pod 代表集群中正在运行的工作负载，并封装一个或多个 Docker 容器，任何必需的存储以及一个唯一的 IP 地址。组成 Pod 的容器在设计上是协同的，并在同一机器上进行调度。\n1.4 Nodes Node 是运行 Kubernetes 集群的机器。它们可以是裸机，虚拟机或其他东西。主机 (Host) 一词通常与 Node 交替使用。我会尽量使用一致的术语 Node，但有时会根据上下文使用虚拟机 (Virtual Machine) 一词来指代节点。\n2 Kubernetes 网络模型 Kubernetes 对 Pods 的联网方式做出了固执的选择。特别的，Kubernetes 对所有的网络实现都作出了以下要求：\n 所有 Pod 都可以与所有其他 Pod 通信而无需使用网络地址转换 (NAT) 所有 Node 都可以在没有 NAT 的情况下与所有 Pod 通信 Pod 所见的它的 IP 就是其他 Pod 看到的它的 IP  由于这些约束，我们现在有四个不同的联网问题需要解决：\n Container 到 Container 网络 Pod 到 Pod 网络 Pod 到 Service 网络 Internet 到 Service 网络  指南剩下的部分将会依次讨论这些问题及其解决方法。\n3 Container 到 Container 网络 通常，我们将虚拟机中的网络通信视为直接与以太网设备进行交互，如图1所示。\n图1. 以太网设备的理想状况\n在现实中，情况更加微妙。在 Linux，每个运行中的进程通过 网络命名空间 进行通信，网络命名空间拥有它自己的路由规则、防火墙规则和网络设备。本质上，一个网络命名空间为命名空间内的所有进程提供了一个全新的网络栈。\nLinux 用户可以用 ip 命令创造网络命名空间。例如，下面的命令创建了一个新的网络命名空间 ns1。\n1  $ ip netns add ns1   当命名空间被创建时，它的挂载点 /var/run/netns 随之创建。这样即使没有进程归属于它也可以持久化。\n你可以通过列出 /var/run/netns 挂载点来列出可用的命名空间，或者用 ip 命令。\n1 2 3 4  $ ls /var/run/netns ns1 $ ip netns ns1   默认的，Linux 将所有进程分配给 root 网络命名空间来提供外部访问，如图2。\n图2. Root 网络命名空间\n对于 Docker 的结构来说，一个 Pod 是一组共享同一网络命名空间的 Docker 容器。同一 Pod 中的容器拥有网络命名空间分配给 Pod 的相同的 IP 地址和端口范围，并且可以通过 localhost 找到各自，因为他们在同一命名空间内。我们为虚拟机中的每一个 Pod 创建网络命名空间。在 Docker 的实现中，一个 \u0026ldquo;Pod 容器\u0026rdquo; 打开网络命名空间，然后用户指定的 \u0026ldquo;app containers\u0026rdquo; 通过 Docker 的 –net=container: 功能加入这个网络命名空间。图3展示了多个容器 (ctr*) 组成的 Pod 在共享的命名空间的情景。\n图3. 每个 Pod 一个网络命名空间\n同一个 Pod 中的应用程序同样拥有共享卷的访问权，根据 Pod 的定义, 共享卷可挂载到每个应用程序的文件系统。\n4 Pod 到 Pod 网络 在 Kubernetes 中，每个 Pod 都拥有真正的 IP 地址而且可以通过这个 IP 地址与别的 Pod 通信。当前需要理解 Kubernetes 是如何使 Pod 到 Pod 通信通过真正的 IP，无论 Pod 是在同一台物理 Node 上还是集群中的不同 Node 上。我们从假设 Pod 在相同的机器上开始讨论，避免 Node 之间通信在内部网络之外的复杂情况。\n对于 Pod 来说，它在其命名空间中尝试去和在同一 Node 上不同网络命名空间通信。幸运的，命名空间可以通过 Linux Virtual Ethernet Device，或通过由两个虚拟接口组成的可以扩展到多个网络命名空间的 veth pair 连接。为了连接 Pod 的命名空间，我们可以把 veth pair 的一端连接到 root 网络命名空间，另一端连接到 Pod 的网络命名空间。Veth pair 像跳线一样工作，连接两端使流量可以流通。这一步可以重复直到其数目和机器上的 Pods 一样多。图4展示了 veth pair 连接了所有 Pod 到 VM 的 root 命名空间的情况。\n图4. 每个 Pod 均有 veth pair\n现在，我们建立了有独立网络命名空间的 Pod，让他们相信他们拥有他们自己的以太网设备和 IP 地址，然后他们连接到了 Node 的 root 命名空间上。现在，我们想让 Pod 通过 root 命名空间和彼此通信，为此我们使用网桥。\nLinux 以太网网桥是一个用来连接两个或以上网段的虚拟的2层网络设备，透明地工作在两个网络上使其相连。网桥维护一个来源和目标的转发表，通过检测经过它的数据包的目的地来决定它是否将数据包传递给别的连接到桥上的网段。网桥码通过查找网络中以太网设备唯一的 MAC 地址来决定了是否桥接数据或将其丢弃。\n网桥实现了 ARP 协议来发现链路层 MAC 地址关联的 IP 地址的。当数据帧被网桥接收时，网桥向所有连接的设备(除了原发送者)广播帧，响应的设备被存入查询表。之后带有相同 IP 地址的流量将会使用查询表来查找转发的正确 MAC 地址。\n图5. 通过网桥连接命名空间\n4.1 包的生命周期：同一 Node 上 Pod 到 Pod 已有将 Pod 隔离到其网络堆栈的网络名称空间，将每个命名空间连接到 root 命名空间的虚拟以太网设备，和将命名空间连接在一起的网桥，我们终于准备好在同一 Node 上的 Pod 之间发送流量了。图6演示了这个过程。\n图6. 包在同一个 Node 上 Pod 之间传递\n在图6中，容器1将数据包发送到其自己的以太网设备 eth0，该设备作为容器的默认设备。对于 Pod1，eth0 通过虚拟以太网设备连接到 root 命名空间 veth0 (1)。配置网桥 cbr0 连接到网段 veth0。数据包到达网桥后，网桥使用 ARP 协议将其解析到其正确的目标网段 veth1 (3)。当数据包到达虚拟设备 veth1 时，它将直接被转发到 Pod2 的命名空间和该命名空间中的 eth0 设备 (4)。在整个流量流中，每个 Pod 仅与 localhost 上的 eth0 通信，并且流量被路由到正确的 Pod。对于这个网络的使用，我们的经验是它符合开发人员期望的默认行为。\nKubernetes的网络模型要求 Pod 必须通过其在跨 Node 中的 IP 地址才能访问。也就是说，一个 Pod 的 IP 地址始终对网络中的其他 Pod 可见，并且每个 Pod 所见的自己的 IP 地址都与其他 Pod 所见一致。现在，我们转向在不同 Node 上的 Pod 之间路由流量的问题。\n4.2 包的生命周期：不同 Node 上 Pod 到 Pod 在确定了如何在同一 Node 上的 Pod 之间路由数据包之后，我们转向在不同 Node 上的 Pod 之间路由流量。Kubernetes 网络模型要求 Pod IP 在整个网络上可达，但是它没有指定必须如何完成。实际上，这是基于特定于网络的，但是已有一些模式被建立起来以简化此过程。\n通常，群集中的每个节点都分配有一个 CIDR 块，该块指定了该 Node 上运行的 Pod 可用的 IP 地址。一旦发往 CID R块的流量到达 Node，则 Node 有责任将流量转发到正确的 Pod。图7说明了两个 Node 之间的流量流，假设网络可以将 CIDR 块中的流量路由到正确的 Node。\n图7. 包在不同 Node 上的 Pod 之间传递\n图7从与图6相同的请求开始，除了目标 Pod (以绿色突出显示)与源 Pod (以蓝色突出显示)位于不同的 Node 上。数据包首先通过 Pod1 的以太网设备发送，该设备与 root 命名空间中的虚拟以太网设备配对 (1)。数据包最终到达 root 名称空间的网桥 (2)。 ARP 将在网桥上失败，因为没有任何匹配数据包 MAC 地址的设备连接到网桥。网桥在失败时会将数据包送出默认路由 - root 命名空间 eth0 设备。此时，路由离开节点并进入网络 (3)。现在我们假设网络可以根据分配给 Node 的 CIDR 块将数据包路由到正确的 Node (4)。数据包进入目标 Node 的 root 命名空间 (VM 2上的 eth0)，然后通过网桥路由到正确的虚拟以太网设备 (5)。最后，通过流经 Pod4 命名空间中的虚拟以太网设备对来完成路由 (6)。一般而言，每个 Node 都知道如何将数据包传递到其中运行的 Pod。数据包到达目标 Node 后，数据包的流动方式与在同一 Node 上的 Pod 之间路由流量的方式相同。\n我们省略了一步，即如何配置网络以将 Pod IP 的流量路由到负责这些 IP 的正确的 Node。这是特定于网络的，不过通过研究相关特例也可以提供其所涉及问题的一些见解。例如，对于 AWS，Amazon 为 Kubernetes 维护了一个容器网络插件，该插件通过 容器网络接口(CNI)插件 使在 Amazon VPC 环境中的 Node 到 Node 网络成为可能。\n容器网络接口(CNI)提供了用于将容器连接到外部网络的通用 API。作为开发人员，我们想知道 Pod 可以使用 IP 地址与网络通信，并且我们希望此操作的机制透明。由 AWS 开发的 CNI 插件试图满足这些需求，同时通过 AWS 提供的现有 VPC，IAM 和安全组功能提供安全且可管理的环境。解决方案是使用弹性网络接口.\n在 EC2 中，每个实例都绑定到一个弹性网络接口(ENI)，并且所有 ENI 都连接在 VPC 内 - ENI 可以相互访问，而无需付出额外的代价。默认情况下，每个 EC2 实例都部署有一个 ENI，但是您可以随意创建多个 ENI 并将它们部署到您认为合适的 EC2 实例中。用于 Kubernetes 的 AWS CNI 插件利用这种灵活性，为部署到 Node 的每一个 Pod 创建一个新的 ENI。由于 VPC 中的 ENI 已经连接到了现有的 AWS 基础设施，因此，每个在 VPC 中 Pod 的 IP 地址都是本地可寻址的。将 CNI 插件部署到群集后，每个 Node (EC2实例) 都会创建多个弹性网络接口，并为这些实例分配 IP 地址，从而为每个 Node 形成一个 CIDR 块。部署 Pod 时，作为 DaemonSet 部署到 Kubernetes 集群的小型二进制文件会收到所有的来自 Node 本地 kubelet 进程的将 Pod 添加到网络的请求。该二进制文件从 Node 的可用 ENI 池中选择一个可用 IP 地址，并通过在 Linux 内核中连接虚拟以太网设备和网桥，将其分配给 Pod，如在同一节点内将 Pod 联网时所述。有了这个，Pod 流量就可以在集群中跨 Node 进行路由。\n5 Pod 到 Service 网络 我们已经展示了如何在 Pod 及其关联的 IP 地址之间路由流量。这非常有效，直到我们需要应对变化。 Pod IP 地址不是持久性的，并且会出现和消失，以应对规模扩大和缩小，应用程序的崩溃或节点重新启动。每一个这些事件都可以使 Pod IP 地址更改而不会发出任何警告。内置在 Kubernetes 的 Service 用以解决此问题。\nKubernetes 的 Service 管理一组 Pod 的状态，使您可以跟踪随时动态变化的一组 Pod IP 地址。Service 充当 Pod 的抽象，并为一组 Pod IP 地址分配一个虚拟 IP 地址。寻址到 Service 的虚拟 IP 的任何流量都将被路由到与虚拟 IP 关联的 Pod 集。这允许与 Service 关联的 Pod 集随时更改 - 客户端只需要了解 Service 的不变的虚拟 IP。\n创建新的 Kubernetes Service 时，将为您创建一个新的虚拟IP (也称为群集IP)。在群集中的任何位置，寻址到虚拟 IP 的流量将负载均衡到与该 Service 关联的一组后端 Pod。实际上，Kubernetes 会自动创建并维护一个集群内的分布式负载均衡器，该负载均衡器会将流量分发到与 Service 相关联的健康 Pod。让我们仔细看看它是如何工作的。\n5.1 netfilter 和 iptables\n为了在群集内执行负载平衡，Kubernetes 依赖于 Linux 内置的网络框架 netfilter。Netfilter 是一个 Linux 提供的网络框架，允许通过定制的 handler 来实现一系列网络相关的操作。 Netfilter 提供了用于数据包过滤，网络地址转换和端口映射的各种功能和操作，这些功能和操作满足了网络中数据包重定向所需，并提供了禁止数据包到达计算机网络内敏感位置的功能。\nIptables 是一个用户空间程序，它提供一个基于表的系统，用于定义使用 netfilter 框架操作和转换数据包的规则。在Kubernetes 中，iptables 规则由监视 Kubernetes API 服务器更改的 kube-proxy Controller 配置。当对 Service 或 Pod 的更改更新了 Service 的虚拟 IP 地址或Pod 的 IP 地址时，将更新 iptables 规则以将针对 Service 的流量正确路由到后端 Pod。 Iptables 规则监视发往 Service的虚拟 IP 的流量，并在匹配项中从可用 Pod 的集合中选择一个随机的 Pod IP 地址，并且 iptables 规则将数据包的目标 IP 地址从服务的虚拟 IP 更改为 选中的 Pod 的 IP。随着 Pod 的规模扩大和缩小，iptables 规则集将更新以反映集群的变化状态。换句话说，iptables 在计算机上已通过将定向到 Service IP 的流量定向到实际 Pod 的 IP 完成了负载均衡，以。\n在反过来的路径上，IP 地址来自目标 Pod。在这种情况下，iptables 再次重写 IP 头，用 Servicce 的 IP 替换 Pod 的 IP，使 Pod 认为它一直在与 Service 的 IP 进行通信。\n5.2 IPVS Kubernetes 的最新版本(1.11)包括集群负载均衡的第二个选项：IPVS。 IPVS (IP虚拟服务器) 也建立在 netfilter 之上，并作为 Linux 内核的一部分实现传输层负载均衡。 IPVS 已集成到 LVS (Linux 虚拟服务器) 中，在主机上运行，并充当真实服务器集群前面的负载均衡器。 IPVS 可以将基于 TCP 和 UDP 的服务的请求定向到真实服务器，并使由多台服务器组成的服务表现为在单个 IP 地址上的虚拟服务。这使得 IPVS 非常适合 Kubernetes Services。\n声明 Kubernetes Service 时，可以指定是否要使用 iptables 或 IPVS 完成集群负载均衡。 IPVS 专为负载均衡而设计，并使用更有效的数据结构 (哈希表)，与 iptables 相比，几乎可以无限扩展。创建使用 IPVS 负载均衡的 Service 时，会发生三件事：在 Node 上创建一个虚拟 IPVS 接口，将 Service 的 IP 地址绑定到该虚拟 IPVS 接口，并为每个 Service IP 地址创建 IPVS 服务器。\n将来，IPVS 有望成为群集内负载均衡的默认方法。这个更改仅影响群集内负载均衡，在本指南的其余部分中，您可以使用 IPVS 安全地将 iptables 替换为群集内负载平衡，而不会影响其余的讨论。现在，让我们看一下通过群集内负载平衡 Service 的数据包的生命周期。\n5.3 包的生命周期：Pod 到 Service Figure 8. 数据包在 Pod 和 Service 间移动\nWhen routing a packet between a Pod and Service, the journey begins in the same way as before. The packet first leaves the Pod through the eth0 interface attached to the Pod’s network namespace (1). Then it travels through the virtual Ethernet device to the bridge (2). The ARP protocol running on the bridge does not know about the Service and so it transfers the packet out through the default route — eth0 (3). Here, something different happens. Before being accepted at eth0, the packet is filtered through iptables. After receiving the packet, iptables uses the rules installed on the Node by kube-proxy in response to Service or Pod events to rewrite the destination of the packet from the Service IP to a specific Pod IP (4). The packet is now destined to reach Pod 4 rather than the Service’s virtual IP. The Linux kernel’s conntrack utility is leveraged by iptables to remember the Pod choice that was made so future traffic is routed to the same Pod (barring any scaling events). In essence, iptables has done in-cluster load balancing directly on the Node. Traffic then flows to the Pod using the Pod-to-Pod routing we’ve already examined (5).\n在Pod和Service之间路由数据包时，旅程以与以前相同的方式开始。数据包首先通过附加到Pod网络名称空间（1）的eth0接口离开Pod。然后，它通过虚拟以太网设备到达网桥（2）。在网桥上运行的ARP协议不了解服务，因此它通过默认路由eth0（3）传送数据包。在这里，发生了一些不同的事情。在被eth0接受之前，该数据包已通过iptables进行过滤。收到数据包后，iptables会使用kube代理安装在节点上的规则来响应Service或Pod事件，将数据包的目标从Service IP重写到特定的Pod IP（4）。现在，该数据包将到达Pod 4，而不是服务的虚拟IP。 iptables充分利用了Linux内核的conntrack实用程序，以记住做出的Pod选择，以便将来的流量被路由到相同的Pod（除非有任何扩展事件）。本质上，iptables直接在节点上完成了集群负载平衡。然后，使用我们已经检查过的Pod到Pod路由，流量流向Pod。\n5.4 Life of a packet: Service to Pod service-to-pod.gif Figure 9. Packets moving between Services and Pods.\nThe Pod that receives this packet will respond, identifying the source IP as its own and the destination IP as the Pod that originally sent the packet (1). Upon entry into the Node, the packet flows through iptables, which uses conntrack to remember the choice it previously made and rewrite the source of the packet to be the Service’s IP instead of the Pod’s IP (2). From here, the packet flows through the bridge to the virtual Ethernet device paired with the Pod’s namespace (3), and to the Pod’s Ethernet device as we’ve seen before (4).\n接收到此数据包的Pod将做出响应，将源IP标识为自己的IP，将目标IP标识为最初发送该数据包的Pod（1）。进入节点后，数据包流经iptables，后者使用conntrack记住其先前所做的选择，并将数据包的源重写为服务的IP，而不是Pod的IP（2）。数据包从这里流过网桥，到达与Pod的命名空间配对的虚拟以太网设备（3），再到我们之前所见的Pod的以太网设备（4）。\n5.5 Using DNS\nKubernetes can optionally use DNS to avoid having to hard-code a Service’s cluster IP address into your application. Kubernetes DNS runs as a regular Kubernetes Service that is scheduled on the cluster. It configures the kubelets running on each Node so that containers use the DNS Service’s IP to resolve DNS names. Every Service defined in the cluster (including the DNS server itself) is assigned a DNS name. DNS records resolve DNS names to the cluster IP of the Service or the IP of a POD, depending on your needs. SRV records are used to specify particular named ports for running Services.\nKubernetes可以选择使用DNS，以避免必须将服务的群集IP地址硬编码到您的应用程序中。 Kubernetes DNS作为在群集上计划的常规Kubernetes服务运行。它配置在每个节点上运行的kubelet，以便容器使用DNS服务的IP来解析DNS名称。为群集中定义的每个服务（包括DNS服务器本身）分配一个DNS名称。 DNS记录根据您的需要将DNS名称解析为服务的群集IP或POD的IP。 SRV记录用于指定运行服务的特定命名端口。\nA DNS Pod consists of three separate containers:\nkubedns: watches the Kubernetes master for changes in Services and Endpoints, and maintains in-memory lookup structures to serve DNS requests. dnsmasq: adds DNS caching to improve performance. sidecar: provides a single health check endpoint to perform healthchecks for dnsmasq and kubedns.\nkubedns：监视Kubernetes主服务器的服务和端点更改，并维护内存查找结构以服务DNS请求。 dnsmasq：添加DNS缓存以提高性能。 sidecar：提供单个运行状况检查端点，以执行dnsmasq和kubedns的运行状况检查。\nThe DNS Pod itself is exposed as a Kubernetes Service with a static cluster IP that is passed to each running container at startup so that each container can resolve DNS entries. DNS entries are resolved through the kubedns system that maintains in-memory DNS representations. etcd is the backend storage system for cluster state, and kubedns uses a library that converts etcd key-value stores to DNS entires to rebuild the state of the in-memory DNS lookup structure when necessary.\nDNS Pod本身作为Kubernetes服务公开，具有静态群集IP，该IP在启动时传递给每个正在运行的容器，以便每个容器都可以解析DNS条目。 DNS条目通过kubedns系统解析，该系统在内存中维护DNS表示形式。 etcd是用于群集状态的后端存储系统，而kubedns使用一个库，该库在必要时将etcd密钥值存储转换为DNS整体，以重建内存DNS查找结构的状态。\nCoreDNS works similarly to kubedns but is built with a plugin architecture that makes it more flexible. As of Kubernetes 1.11, CoreDNS is the default DNS implementation for Kubernetes.\nCoreDNS的工作方式与kubedns相似，但其使用的插件体系结构使其更加灵活。从Kubernetes 1.11开始，CoreDNS是Kubernetes的默认DNS实现。\n6 Internet-to-Service Networking\nSo far we have looked at how traffic is routed within a Kubernetes cluster. This is all fine and good but unfortunately walling off your application from the outside world will not help meet any sales targets — at some point you will want to expose your service to external traffic. This need highlights two related concerns: (1) getting traffic from a Kubernetes Service out to the Internet, and (2) getting traffic from the Internet to your Kubernetes Service. This section deals with each of these concerns in turn.\n到目前为止，我们已经研究了如何在Kubernetes集群中路由流量。一切都很好，但是不幸的是，与外界隔离您的应用程序将无助于达到任何销售目标-在某个时候，您将需要向外部流量公开您的服务。这种需求突出了两个相关的问题：（1）将来自Kubernetes服务的流量发送到Internet，以及（2）将来自Internet的流量发送到您的Kubernetes Service。本节依次阐述这些问题。\n6.1 Egress — Routing traffic to the Internet\nRouting traffic from a Node to the public Internet is network specific and really depends on how your network is configured to publish traffic. To make this section more concrete I will use an AWS VPC to discuss any specific details.\n从节点到公共Internet的流量路由是特定于网络的，并且实际上取决于网络配置为发布流量的方式。为了使本节更具体，我将使用AWS VPC讨论任何特定细节。\nIn AWS, a Kubernetes cluster runs within a VPC, where every Node is assigned a private IP address that is accessible from within the Kubernetes cluster. To make traffic accessible from outside the cluster, you attach an Internet gateway to your VPC. The Internet gateway serves two purposes: providing a target in your VPC route tables for traffic that can be routed to the Internet, and performing network address translation (NAT) for any instances that have been assigned public IP addresses. The NAT translation is responsible for changing the Node’s internal IP address that is private to the cluster to an external IP address that is available in the public Internet.\n在AWS中，Kubernetes集群在VPC内运行，其中为每个节点分配了一个私有IP地址，该地址可从Kubernetes集群内访问。要使流量可以从群集外部访问，请将Internet网关连接到VPC。 Internet网关有两个目的：在VPC路由表中为可路由到Internet的流量提供目标，并为已分配了公共IP地址的任何实例执行网络地址转换（NAT）。NAT转换负责将群集专用的节点内部IP地址更改为公用Internet上可用的外部IP地址。\nWith an Internet gateway in place, VMs are free to route traffic to the Internet. Unfortunately, there is a small problem. Pods have their own IP address that is not the same as the IP address of the Node that hosts the Pod, and the NAT translation at the Internet gateway only works with VM IP addresses because it does not have any knowledge about what Pods are running on which VMs — the gateway is not container aware. Let’s look at how Kubernetes solves this problem using iptables (again).\n有了Internet网关后，VM可以自由地将流量路由到Internet。不幸的是，这是一个小问题。 Pod拥有自己的IP地址，该IP地址与托管Pod的节点的IP地址不同，并且Internet网关上的NAT转换仅适用于VM IP地址，因为它不了解运行Pod的任何知识哪些虚拟机-网关不支持容器。让我们看看Kubernetes如何使用iptables解决这个问题（再次）。\n6.1.1 Life of a packet: Node to Internet\nIn the following diagram, the packet originates at the Pod’s namespace (1) and travels through the veth pair connected to the root namespace (2). Once in the root namespace, the packet moves from the bridge to the default device since the IP on the packet does not match any network segment connected to the bridge. Before reaching the root namespace’s Ethernet device (3), iptables mangles the packet (3). In this case, the source IP address of the packet is a Pod, and if we keep the source as a Pod the Internet gateway will reject it because the gateway NAT only understands IP addresses that are connected to VMs. The solution is to have iptables perform a source NAT — changing the packet source — so that the packet appears to be coming from the VM and not the Pod. With the correct source IP in place, the packet can now leave the VM (4) and reach the Internet gateway (5). The Internet gateway will do another NAT rewriting the source IP from a VM internal IP to an external IP. Finally, the packet will reach the public Internet (6). On the way back, the packet follows the same path and any source IP mangling is undone so that each layer of the system receives the IP address that it understands: VM-internal at the Node or VM level, and a Pod IP within a Pod’s namespace.\n在下图中，数据包起源于Pod的名称空间（1），并经过与根名称空间（2）连接的第ve对。一旦进入根名称空间，数据包就会从网桥移动到默认设备，因为数据包上的IP与连接到网桥的任何网段都不匹配。在到达根名称空间的以太网设备（3）之前，iptables会处理数据包（3）。在这种情况下，数据包的源IP地址是Pod，如果我们将源保留为Pod，则Internet网关将拒绝它，因为网关NAT仅了解连接到VM的IP地址。解决方案是让iptables执行源NAT（更改数据包源），以便数据包看起来是来自VM而不是Pod。有了正确的源IP，数据包现在可以离开VM（4）并到达Internet网关（5）。 Internet网关将执行另一个NAT，将源IP从VM内部IP重写为外部IP。最终，数据包将到达公共Internet（6）。在返回的过程中，数据包遵循相同的路径，并且所有源IP处理都被撤消，因此系统的每一层都接收其能够理解的IP地址：节点或VM级别的内部VM，以及Pod命名空间中的Pod IP 。\npod-to-internet.gif Figure 10. Routing packets from Pods to the Internet.\n6.2 Ingress — Routing Internet traffic to Kubernetes\nIngress — getting traffic into your cluster — is a surprisingly tricky problem to solve. Again, this is specific to the network you are running, but in general, Ingress is divided into two solutions that work on different parts of the network stack: (1) a Service LoadBalancer and (2) an Ingress controller.\n入口（将流量引入群集）是一个非常棘手的问题。同样，这是特定于您正在运行的网络的，但是通常，Ingress分为两个可在网络堆栈的不同部分上运行的解决方案：（1）Service LoadBalancer和（2）Ingress控制器。\n6.2.1 Layer 4 Ingress: LoadBalancer\nWhen you create a Kubernetes Service you can optionally specify a LoadBalancer to go with it. The implementation of the LoadBalancer is provided by a cloud controller that knows how to create a load balancer for your service. Once your Service is created, it will advertise the IP address for the load balancer. As an end user, you can start directing traffic to the load balancer to begin communicating with your Service.\n创建Kubernetes服务时，可以选择指定一个LoadBalancer来配合它。云控制器提供了LoadBalancer的实现，该控制器知道如何为您的服务创建负载平衡器。创建服务后，它将为负载平衡器通告IP地址。作为最终用户，您可以开始将流量定向到负载平衡器，以开始与服务进行通信。\nWith AWS, load balancers are aware of Nodes within their Target Group and will balance traffic throughout all of the Nodes in the cluster. Once traffic reaches a Node, the iptables rules previously installed throughout the cluster for your Service will ensure that traffic reaches the Pods for the Service you are interested in.\n借助AWS，负载平衡器可以知道其目标组中的节点，并将平衡群集中所有节点上的流量。一旦流量到达节点，先前在整个群集中为您的服务安装的iptables规则将确保流量到达您感兴趣的服务的Pod。\n6.2.2 Life of a packet: LoadBalancer to Service\nLet’s look at how this works in practice. Once you deploy your service, a new load balancer will be created for you by the cloud provider you are working with (1). Because the load balancer is not container aware, once traffic reaches the load-balancer it is distributed throughout the VMs that make up your cluster (2). iptables rules on each VM will direct incoming traffic from the load balancer to the correct Pod (3) — these are the same IP tables rules that were put in place during Service creation and discussed earlier. The response from the Pod to the client will return with the Pod’s IP, but the client needs to have the load balancer’s IP address. iptables and conntrack is used to rewrite the IPs correctly on the return path, as we saw earlier.\n让我们看看这在实践中是如何工作的。部署服务后，您正在使用的云提供商将为您创建一个新的负载均衡器（1）。由于负载平衡器不支持容器，因此，一旦流量到达负载平衡器，流量便会分布在组成群集的所有VM中（2）。每个VM上的iptables规则会将来自负载均衡器的传入流量定向到正确的Pod（3），这些规则与创建服务期间制定的IP表规则相同，前面已经讨论过。从Pod到客户端的响应将返回Pod的IP，但客户端需要具有负载均衡器的IP地址。如前所述，iptables和conntrack用于在返回路径上正确重写IP。\nThe following diagram shows a network load balancer in front of three VMs that host your Pods. Incoming traffic (1) is directed at the load balancer for your Service. Once the load balancer receives the packet (2) it picks a VM at random. In this case, we’ve chosen pathologically the VM with no Pod running: VM 2 (3). Here, the iptables rules running on the VM will direct the packet to the correct Pod using the internal load balancing rules installed into the cluster using kube-proxy. iptables does the correct NAT and forwards the packet on to the correct Pod (4).\n下图显示了在承载Pod的三个VM之前的网络负载平衡器。传入流量（1）指向服务的负载平衡器。一旦负载均衡器收到数据包（2），它就会随机选择一个VM。在这种情况下，我们从病理上选择了没有运行Pod的VM：VM 2（3）。在这里，在VM上运行的iptables规则将使用通过kube代理安装到群集中的内部负载平衡规则将数据包定向到正确的Pod。 iptables执行正确的NAT，并将数据包转发到正确的Pod（4）。\ninternet-to-service.gif Figure 11. Packets sent from the Internet to a Service. 6.2.3 Layer 7 Ingress: Ingress Controller\nLayer 7 network Ingress operates on the HTTP/HTTPS protocol range of the network stack and is built on top of Services. The first step to enabling Ingress is to open a port on your Service using the NodePort Service type in Kubernetes. If you set the Service’s type field to NodePort, the Kubernetes master will allocate a port from a range you specify, and each Node will proxy that port (the same port number on every Node) into your Service. That is, any traffic directed to the Node’s port will be forwarded on to the service using iptables rules. This Service to Pod routing follows the same internal cluster load-balancing pattern we’ve already discussed when routing traffic from Services to Pods.\n第7层网络入口在网络堆栈的HTTP / HTTPS协议范围内运行，并建立在服务之上。启用Ingress的第一步是使用Kubernetes中的NodePort服务类型在服务上打开端口。如果将服务的类型字段设置为NodePort，则Kubernetes主服务器将在您指定的范围内分配端口，并且每个节点会将该端口（每个节点上的相同端口号）代理到您的服务中。也就是说，使用iptables规则，任何定向到该节点端口的流量都将转发到该服务。从服务到Pod的路由遵循了我们已经讨论过的相同的内部集群负载平衡模式。\nTo expose a Node’s port to the Internet you use an Ingress object. An Ingress is a higher-level HTTP load balancer that maps HTTP requests to Kubernetes Services. The Ingress method will be different depending on how it is implemented by the Kubernetes cloud provider controller. HTTP load balancers, like Layer 4 network load balancers, only understand Node IPs (not Pod IPs) so traffic routing similarly leverages the internal load-balancing provided by the iptables rules installed on each Node by kube-proxy.\n要将节点的端口暴露给Internet，请使用Ingress对象。 Ingress是更高级别的HTTP负载平衡器，可将HTTP请求映射到Kubernetes Services。 Ingress方法将有所不同，具体取决于Kubernetes云提供商控制器如何实现。 HTTP负载平衡器（如第4层网络负载平衡器）仅了解节点IP（而非Pod IP），因此流量路由同样利用kube代理在每个节点上安装的iptables规则提供的内部负载平衡。\nWithin an AWS environment, the ALB Ingress Controller provides Kubernetes Ingress using Amazon’s Layer 7 Application Load Balancer. The following diagram details the AWS components this controller creates. It also demonstrates the route Ingress traffic takes from the ALB to the Kubernetes cluster.\n在AWS环境中，ALB入口控制器使用Amazon的7层应用程序负载平衡器提供Kubernetes入口。下图详细说明了此控制器创建的AWS组件。它还演示了入口流量从ALB到Kubernetes集群的路线。\ningress-controller-design.png Figure 12. Design of an Ingress Controller.\nUpon creation, (1) an Ingress Controller watches for Ingress events from the Kubernetes API server. When it finds Ingress resources that satisfy its requirements, it begins the creation of AWS resources. AWS uses an Application Load Balancer (ALB) (2) for Ingress resources. Load balancers work in conjunction with Target Groups that are used to route requests to one or more registered Nodes. (3) Target Groups are created in AWS for each unique Kubernetes Service described by the Ingress resource. (4) A Listener is an ALB process that checks for connection requests using the protocol and port that you configure. Listeners are created by the Ingress controller for every port detailed in your Ingress resource annotations. Lastly, Target Group Rules are created for each path specified in your Ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service (5).\n创建后，（1）入口控制器将监视来自Kubernetes API服务器的入口事件。当发现满足其要求的Ingress资源时，它将开始创建AWS资源。 AWS将应用程序负载平衡器（ALB）（2）用于Ingress资源。负载均衡器与用于将请求路由到一个或多个已注册节点的目标组配合使用。 （3）在AWS中为Ingress资源描述的每个唯一的Kubernetes服务创建目标组。（4）侦听器是ALB进程，它使用您配置的协议和端口检查连接请求。侦听器是由Ingress控制器为Ingress资源注释中详细说明的每个端口创建的。最后，为Ingress资源中指定的每个路径创建目标组规则。这样可以确保将到特定路径的流量路由到正确的Kubernetes服务（5）。\n6.2.4 Life of a packet: Ingress to Service\nThe life of a packet flowing through an Ingress is very similar to that of a LoadBalancer. The key differences are that an Ingress is aware of the URL’s path (allowing and can route traffic to services based on their path), and that the initial connection between the Ingress and the Node is through the port exposed on the Node for each service.\n流经Ingress的数据包的寿命与LoadBalancer的寿命非常相似。主要区别在于，Ingress知道URL的路径（允许并可以根据服务的路径将流量路由到服务），并且Ingress和Node之间的初始连接是通过Node上每个服务暴露的端口进行的。\nLet’s look at how this works in practice. Once you deploy your service, a new Ingress load balancer will be created for you by the cloud provider you are working with (1). Because the load balancer is not container aware, once traffic reaches the load-balancer it is distributed throughout the VMs that make up your cluster (2) through the advertised port for your service. iptables rules on each VM will direct incoming traffic from the load balancer to the correct Pod (3) — as we have seen before. The response from the Pod to the client will return with the Pod’s IP, but the client needs to have the load balancer’s IP address. iptables and conntrack is used to rewrite the IPs correctly on the return path, as we saw earlier.\n让我们看看这在实践中是如何工作的。部署服务后，您正在使用的云提供商将为您创建一个新的Ingress负载均衡器（1）。由于负载平衡器不支持容器，因此，一旦流量到达负载平衡器，它将通过为您的服务通告的端口在组成您的群集（2）的所有VM中进行分配。如前所述，每个VM上的iptables规则会将来自负载均衡器的传入流量定向到正确的Pod（3）。从Pod到客户端的响应将返回Pod的IP，但客户端需要具有负载均衡器的IP地址。如前所述，iptables和conntrack用于在返回路径上正确重写IP。\ningress-to-service.gif Figure 13. Packets sent from an Ingress to a Service.\nOne benefit of Layer 7 load-balancers are that they are HTTP aware, so they know about URLs and paths. This allows you you to segment your Service traffic by URL path. They also typically provide the original client’s IP address in the X-Forwarded-For header of the HTTP request.\n第7层负载平衡器的一个好处是它们可以识别HTTP，因此他们知道URL和路径。这使您可以按URL路径细分服务流量。它们通常还会在HTTP请求的X Forwarded For标头中提供原始客户端的IP地址。\n7 Wrapping Up This guide provides a foundation for understanding the Kubernetes networking model and how it enables common networking tasks. The field of networking is both broad and deep and it’s impossible to cover everything here. This guide should give you a starting point to dive into the topics you are interested in and want to know more about. Whenever you are stumped, leverage the Kubernetes documentation and the Kubernetes community to help you find your way.\n8 Glossary\nKubernetes relies on several existing technologies to build a functioning cluster. Fully exploring each of these technologies is beyond the scope of this guide, but this section describes each of those technologies in enough detail to follow along with the discussion. You can feel free to skim this section, skip it completely, or refer to it as needed if you ever get confused or need a refresher.\nKubernetes依靠几种现有技术来构建可运行的集群。全面探索每种技术不在本指南的讨论范围内，但是本节将对每种技术进行足够详细的介绍，以供讨论时参考。您可以随意浏览本节，完全跳过本节，或者在感到困惑或需要复习时根据需要参考。\nLayer 2 Networking Layer 2 is the data link layer providing Node-to-Node data transfer. It defines the protocol to establish and terminate a connection between two physically connected devices. It also defines the protocol for flow control between them.\nLayer 4 Networking The transport layer controls the reliability of a given link through flow control. In TCP/IP, this layer refers to the TCP protocol for exchanging data over an unreliable network.\nLayer 7 Networking The application layer is the layer closest to the end user, which means both the application layer and the user interact directly with the software application. This layer interacts with software applications that implement a communicating component. Typically, Layer 7 Networking refers to HTTP.\nNAT — Network Address Translation NAT or network address translation is an IP-level remapping of one address space into another. The mapping happens by modifying network address information in the IP header of packets while they are in transit across a traffic routing device.\nA basic NAT is a simple mapping from one IP address to another. More commonly, NAT is used to map multiple private IP address into one publicly exposed IP address. Typically, a local network uses a private IP address space and a router on that network is given a private address in that space. The router is then connected to the Internet with a public IP address. As traffic is passed from the local network to the Internet, the source address for each packet is translated from the private address to the public address, making it seem as though the request is coming directly from the router. The router maintains connection tracking to forward replies to the correct private IP on the local network.\nNAT provides an additional benefit of allowing large private networks to connect to the Internet using a single public IP address, thereby conserving the number of publicly used IP addresses.\nSNAT — Source Network Address Translation SNAT simply refers to a NAT procedure that modifies the source address of an IP packet. This is the typical behaviour for the NAT described above.\nDNAT — Destination Network Address Translation DNAT refers to a NAT procedure that modifies the destination address of an IP packet. DNAT is used to publish a service resting in a private network to a publicly addressable IP address.\nNetwork Namespace In networking, each machine (real or virtual) has an Ethernet device (that we will refer to as eth0). All traffic flowing in and out of the machine is associated with that device. In truth, Linux associates each Ethernet device with a network namespace — a logical copy of the entire network stack, with its own routes, firewall rules, and network devices. Initially, all the processes share the same default network namespace from the init process, called the root namespace. By default, a process inherits its network namespace from its parent and so, if you don’t make any changes, all network traffic flows through the Ethernet device specified for the root network namespace.\nveth — Virtual Ethernet Device Pairs Computer systems typically consist of one or more networking devices — eth0, eth1, etc — that are associated with a physical network adapter which is responsible for placing packets onto the physical wire. Veth devices are virtual network devices that are always created in interconnected pairs. They can act as tunnels between network namespaces to create a bridge to a physical network device in another namespace, but can also be used as standalone network devices. You can think of a veth device as a virtual patch cable between devices — what goes in one end will come out the other.\nbridge — Network Bridge A network bridge is a device that creates a single aggregate network from multiple communication networks or network segments. Bridging connects two separate networks as if they were a single network. Bridging uses an internal data structure to record the location that each packet is sent to as a performance optimization.\nCIDR — Classless Inter-Domain Routing CIDR is a method for allocating IP addresses and performing IP routing. With CIDR, IP addresses consist of two groups: the network prefix (which identifies the whole network or subnet), and the host identifier (which specifies a particular interface of a host on that network or subnet). CIDR represents IP addresses using CIDR notation, in which an address or routing prefix is written with a suffix indicating the number of bits of the prefix, such as 192.0.2.0/24 for IPv4. An IP address is part of a CIDR block, and is said to belong to the CIDR block if the initial n bits of the address and the CIDR prefix are the same.\nCNI — Container Network Interface CNI (Container Network Interface) is a Cloud Native Computing Foundation project consisting of a specification and libraries for writing plugins to configure network interfaces in Linux containers. CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted.\nVIP — Virtual IP Address A virtual IP address, or VIP, is a software-defined IP address that doesn’t correspond to an actual physical network interface.\nnetfilter — The Packet Filtering Framework for Linux netfilter is the packet filtering framework in Linux. The software implementing this framework is responsible for packet filtering, network address translation (NAT), and other packet mangling.\nnetfilter, ip_tables, connection tracking (ip_conntrack, nf_conntrack) and the NAT subsystem together build the major parts of the framework.\niptables — Packet Mangling Tool iptables is a program that allows a Linux system administrator to configure the netfilter and the chains and rules it stores. Each rule within an IP table consists of a number of classifiers (iptables matches) and one connected action (iptables target).\nconntrack — Connection Tracking conntrack is a tool built on top of the Netfilter framework to handle connection tracking. Connection tracking allows the kernel to keep track of all logical network connections or sessions, and direct packets for each connection or session to the correct sender or receiver. NAT relies on this information to translate all related packets in the same way, and iptables can use this information to act as a stateful firewall.\nIPVS — IP Virtual Server IPVS implements transport-layer load balancing as part of the Linux kernel.\nIPVS is a tool similar to iptables. It is based on the Linux kernel’s netfilter hook function, but uses a hash table as the underlying data structure. That means, when compared to iptables, IPVS redirects traffic much faster, has much better performance when syncing proxy rules, and provides more load balancing algorithms.\nDNS — The Domain Name System The Domain Name System (DNS) is a decentralized naming system for associating system names with IP addresses. It translates domain names to numerical IP addresses for locating computer services.\n","date":"2019-10-28T18:55:42+08:00","permalink":"https://memwey.github.io/p/%E7%BF%BB%E8%AF%91-kubernetes-%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E6%8C%87%E5%8D%97/","title":"[翻译] Kubernetes 网络模型指南"},{"content":"又是关于 MySQL 的, 标题来自于 \u0026lt;轮到你了\u0026gt; 这部烂尾日剧, 里面非常魔性的 Oh~my~Ju~lia~\n在MySQL里面如何保存 Emoji, 这个问题搜一搜很容易找到答案, 设置 CHARSET 为 utf8mb4, 看来天下苦 MySQL 久矣.\n基础知识 计算机最早是美国人发明的, 那时的编码都是 ASCII 码, 毕竟英文字符加上标点符号也就那么多嘛, 一个字节绰绰有余.\n但是随着计算机的发展, 计算机面向的国家和语言越来越多, 一个字节根本不够用了, 于是就有了很多面向特定语言的编码, 比如汉字的GBK, Big5, 韩文的 EUC-KR, 日文的 Shift_JIS 什么的. 总体思想是, 既然一个字节不够用, 那就多几个字节就是了嘛.\n但是问题又出现了, 同一个编码在不同语言中会有不同的含义,比如韩文编码 EUC-KR 中 한국어 的编码值正好对应着汉字编码 GBK 中的 茄惫绢. 还有那个著名的 瓣B变巨肚, 也正是同样的原因. 这样仿佛就变成了巴别塔的故事. 人们虽然用着同样的二进制编码, 但是却表达着不同的意思; 就像人们虽然同样发出声带的震动, 但是却无法互相理解对方的话语.\n国际标准化组织 和 统一码联盟 意识到, 这样下去是不行的, 不如搞一个超大的字符集, 然后把人类所有字符都弄进去, 这样人类都可以用同一个标准了. 于是, 他们分别制定了 USC 和 Unicode. 当然, 如果两个标准各搞各的, 那就和他们最初的想法背道而驰了, 所以目前这两个字符集在实际使用中是一致的. 一般还是把这个字符集叫 Unicode.\nUnicode 的范围上限是 0x10FFFF, 换算成十进制就是 1,114,111 这么多. 所以如果以后外星人的文字太多, 说不定这个范围就不够了.\n其实这里悄悄的偷换了一个概念, 之前在说编码, 现在变成了字符集了. 在很多之前提到的编码中, 这些编码既是字符集, 又是直接的编码. 而在 Unicode 中, 字符集实际上是与编码分开的两个概念, 而对应的编码, 实际上就是我们经常见到的 UTF. 其中 UTF-8 是我们最常见的一种编码了.\nUTF-8 最大的特点, 就是它是一种变长的编码方式, 而且完全兼容 ASCII码.\nUTF-8 的编码规则也很简单，只有二条:\n  对于单字节的符号, 字节的第一位设为0, 后面7位为这个符号的 Unicode 码. 对于 0 - 127 位的字符，UTF-8 编码和 ASCII 码是完全相同的.\n  对于 n 字节的符号 (n \u0026gt; 1) , 第一个字节的前 n 位都设为 1, 第 n + 1 位设为 0, 后面字节的前两位一律设为 10. 剩下的没有提及的二进制位, 全部为这个符号的 Unicode 码.\n     Unicode符号范围 UTF-8编码方式     U+0000 - U+007F 0xxxxxxx   U+0080 - U+07FF 110xxxxx 10xxxxxx   U+0800 - U+FFFF 1110xxxx 10xxxxxx 10xxxxxx   U+010000 - U+10FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx    在这里我们发现, U+0000 - U+FFFF 这个平面(Plane), 即基本多文种平面, 简称 BMP, 用 UTF-8 进行编码, 只需要三个字节.\n轮到你了 在 MySQL 的 CHARSET 中, utf8 支持存储 1 - 3字节的字符, 即对应 Unicode 中 BMP 的部分, 而 Emoji, 则大多数在 BMP 之外. 所以 MySQL 中的 utf8 是假的, 是化学的成分, 是加了特技的. 如果想储存真正的 UTF-8 的内容, 就一定要使用 utf8mb4.\n当然, 并不是所有的 Emoji 都在 BMP 之外, 比如 ☺ 这个表情, 它的编码是 U+263A, 还有 ☹️ , 编码是 U+2639.\n当然, 也不是所有的汉字都被包含在了 BMP 里面了, 在 这里 可以看到字符是按照一定的规律划分为 Block 放进来的, 在表意文字补充平面也就是 U+20000 - U+2FFFF 这个平面(Plane), 还是有很多 CJK 命名的 Block 的.\n再来一瓶 你以为坑到这里就结束了吗\n1 2 3 4 5  CREATETABLE`emoji_test`(`id`INTUNSIGNEDNOTNULLAUTO_INCREMENT,`emoji`VARCHAR(1024)NOTNULL,PRIMARYKEY(`id`))ENGINE=InnoDBDEFAULTCHARSET=utf8mb4;  1  SETNAMESutf8mb4;  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  MySQL [test]\u0026gt; SELECT * FROM emoji_test; +----+-------+ | id | emoji | +----+-------+ | 8 | 😃 | | 9 | 😂 | | 10 | 🤦 | +----+-------+ 3 rows in set (0.001 sec) MySQL [test]\u0026gt; SELECT * FROM emoji_test WHERE emoji = \u0026#39;😂\u0026#39;; +----+-------+ | id | emoji | +----+-------+ | 8 | 😃 | | 9 | 😂 | | 10 | 🤦 | +----+-------+ 3 rows in set (0.001 sec)   是不是很神奇呢, 看下表的信息呢\n1 2 3 4 5 6 7  MySQL[test]\u0026gt;SHOWTABLESTATUSWHEREName=\u0026#39;emoji_test\u0026#39;;+------------+--------+---------+------------+------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+--------------------+----------+----------------+---------+ |Name|Engine|Version|Row_format|Rows|Avg_row_length|Data_length|Max_data_length|Index_length|Data_free|Auto_increment|Create_time|Update_time|Check_time|Collation|Checksum|Create_options|Comment|+------------+--------+---------+------------+------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+--------------------+----------+----------------+---------+ |emoji_test|InnoDB|10|Dynamic|3|5461|16384|0|0|0|11|2019-10-1611:16:28|2019-10-1611:25:10|NULL|utf8mb4_general_ci|NULL|||+------------+--------+---------+------------+------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+---------------------+------------+--------------------+----------+----------------+---------+ 1rowinset(0.001sec)  可以看到此时默认的字符序是 utf8mb4_general_ci, 使用 WEIGHT_STRING 来查询这些 Emoji 的字符序\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  MySQL[test]\u0026gt;SET@s=\u0026#39;😂\u0026#39;COLLATEutf8mb4_general_ci;QueryOK,0rowsaffected(0.003sec)MySQL[test]\u0026gt;SELECT@s,HEX(@s),HEX(WEIGHT_STRING(@s));+------+----------+------------------------+ |@s|HEX(@s)|HEX(WEIGHT_STRING(@s))|+------+----------+------------------------+ |😂|F09F9882|FFFD|+------+----------+------------------------+ 1rowinset(0.001sec)MySQL[test]\u0026gt;SET@s=\u0026#39;😃\u0026#39;COLLATEutf8mb4_general_ci;QueryOK,0rowsaffected(0.001sec)MySQL[test]\u0026gt;SELECT@s,HEX(@s),HEX(WEIGHT_STRING(@s));+------+----------+------------------------+ |@s|HEX(@s)|HEX(WEIGHT_STRING(@s))|+------+----------+------------------------+ |😃|F09F9883|FFFD|+------+----------+------------------------+ 1rowinset(0.001sec)  可以看到, 它们的编码虽然不同, 但是字符序的值是相同的, 都是 0xFFFD, 所以在匹配的时候, 它们被认为是同一个字符, 这又是 MySQL 的一个大坑\n要是想粗暴点解决问题的话, 直接使用 utf8mb4_bin 作为字符序就好了\n又来一瓶 但是这还不是结束, 当业务代码连接数据库并插入一些 Emoji 时, 还是可能遇到如下错误\n1  ERROR 1366: Incorrect string value: \u0026#39;\\xF0\\x9D\\x8C\\x86\u0026#39; for column \u0026#39;column_name\u0026#39; at row 1   这个是因为客户端到 MySQL 服务器的连接还是 utf8 而非 utf8mb4, 所以你需要在业务逻辑中做类似如下语句的事情\n1  SET NAMES utf8mb4   或者修改 MySQL 的配置文件, 当然这样不太现实, 需要重启 MySQL 进程.\n这样你才能终于用上 Emoji\n参考资料  How can I search by emoji in MySQL using utf8mb4? 字符编码笔记：ASCII，Unicode 和 UTF-8  如何通俗地理解Unicode、UTF-8、ASCII、GBK等字符编码？ In MySQL, never use “utf8”. Use “utf8mb4”. How to support full Unicode in MySQL databases Connection Character Sets and Collations “Incorrect string value” when trying to insert UTF-8 into MySQL via JDBC?  扩展阅读  其实你并不懂 Unicode Dark corners of Unicode MySQL · 实现分析 · 对字符集和字符序支持的实现 unicodedata — Unicode Database  ","date":"2019-10-14T11:12:33+08:00","permalink":"https://memwey.github.io/p/oh-mysql-emoji/","title":"Oh MySQL Emoji"},{"content":"问题来源 默认情况下,Docker会把东西装在 /var/lib/docker 目录下,当然这个可以通过修改 /etc/docker/daemon.json 中的值来修改.\nDocker在把各种乱七八糟的东西都放在\n","date":"2019-10-08T23:50:02+08:00","permalink":"https://memwey.github.io/p/mount-bind/","title":"Mount Bind"},{"content":"感觉 MySQL 的坑实在是有点多,记录一下这个 MySQL 的坑, 也记录一下这个教训吧, 下次在数据库中直接操作一定要多小心\n现象回放 现在有两张表, 表结构如下, 无关字段已经省略\n team  1 2 3 4 5 6  +-------------+---------------------+ | Field | Type | +-------------+---------------------+ | id | int(10) unsigned | | status | tinyint(3) unsigned | +-------------+---------------------+    player  1 2 3 4 5 6 7  +-------------+---------------------+ | Field | Type | +-------------+---------------------+ | id | int(10) unsigned | | team_id | int(10) unsigned | | status | tinyint(3) unsigned | +-------------+---------------------+   容易理解, 这是简单的一对多的关系, 一个足球队 team 里面, 有多个球员 player\n现在想取出有 player 的状态为 1 的 team\n1  SELECT*FROMteamWHEREidIN(SELECTidFROM(SELECTteam_idFROMplayerWHEREstatus=1)ASa);  理论上, 这条语句是不能执行的, 注意这里\n \u0026hellip;\u0026hellip; SELECT id FROM (SELECT team_id FROM \u0026hellip;\u0026hellip;\n 但是, 不知道为何, 这条语句是可以执行的, 而且等价于\n1  SELECT*FROMteam;  如果单独把最外层的 IN 里面的 subquery 取出来, MySQL 会报错\n1  SELECTidFROM(SELECTteam_idFROMplayerWHEREstatus=1)ASa;   ERROR 1054 (42S22): Unknown column \u0026lsquo;id\u0026rsquo; in \u0026lsquo;field list\u0026rsquo;\n 再试着把 id 改为不存在的字段\n1  SELECT*FROMteamWHEREidIN(SELECTnot_exist_fieldFROM(SELECTteam_idFROMplayerWHEREstatus=1)ASa);   ERROR 1054 (42S22): Unknown column \u0026rsquo;not_exist_field\u0026rsquo; in \u0026lsquo;field list\u0026rsquo;\n 这样才能如预期的报错\n问题排查 先 EXPLAIN 试试呢\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  ***************************1.row***************************id:1select_type:SIMPLEtable:teampartitions:NULLtype:ALLpossible_keys:NULLkey:NULLkey_len:NULLref:NULLrows:2filtered:100.00Extra:NULL***************************2.row***************************id:1select_type:SIMPLEtable:playerpartitions:NULLtype:ALLpossible_keys:NULLkey:NULLkey_len:NULLref:NULLrows:5filtered:20.00Extra:Usingwhere;FirstMatch(team);Usingjoinbuffer(BlockNestedLoop)2rowsinset,2warnings(0.001sec)  好像看不出来什么问题呢, 不过有 warnings, 看一下呢\n1  SHOWWARNINGS\\G  1 2 3 4 5 6 7 8 9  ***************************1.row***************************Level:NoteCode:1276Message:Fieldorreference\u0026#39;test.team.id\u0026#39;ofSELECT#2 was resolved in SELECT #1 ***************************2.row***************************Level:NoteCode:1003Message:/* select#1 */select`test`.`team`.`id`AS`id`,`test`.`team`.`status`AS`status`from`test`.`team`semijoin(`test`.`player`)where(`test`.`player`.`status`=1)2rowsinset(0.001sec)  这里出现了一个 semi join, 没有见过呢, 查看一下文档, 大意就是, 比如使用 INNER JOIN 的时候, 会返回匹配次数个结果. 但是我们并不关注匹配的次数, 比如如下语句, 我想取出有球员的 status 为 0 的球队, 可以这样写\n1  SELECT*FROMteamINNERJOINplayerONteam.id=team_idWHEREplayer.status=0;  如果一个球队里有多个 status 为 0 的球员, 那么就会出现多个记录, 比如这样\n1 2 3 4 5 6 7 8  +----+--------+----+---------+--------+ | id | status | id | team_id | status | +----+--------+----+---------+--------+ | 1 | 0 | 1 | 1 | 0 | | 1 | 0 | 2 | 1 | 0 | | 2 | 0 | 5 | 2 | 0 | | 2 | 0 | 6 | 2 | 0 | +----+--------+----+---------+--------+   这样明显有些冗余的数据了. 当然我们可以用 DISTINCT 什么的再处理一遍, 但是这样效率会比较低. 那么, 就可以用类似的子查询就方便多了\n1  SELECT*FROMteamWHEREidIN(SELECTteam_idFROMplayerWHEREstatus=0);  返回的结果也简洁多了\n1 2 3 4 5 6  +----+--------+ | id | status | +----+--------+ | 1 | 0 | | 2 | 0 | +----+--------+   当然, 要这样优化还是有很多条件的, 林林总总的, 可以去官方文档查看\n看了这么多, 感觉还是和这个问题没什么关系啊, 试着 EXPLAIN 一下正确的语句\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  ***************************1.row***************************id:1select_type:SIMPLEtable:teampartitions:NULLtype:ALLpossible_keys:PRIMARYkey:NULLkey_len:NULLref:NULLrows:2filtered:100.00Extra:NULL***************************2.row***************************id:1select_type:SIMPLEtable:playerpartitions:NULLtype:ALLpossible_keys:NULLkey:NULLkey_len:NULLref:NULLrows:5filtered:20.00Extra:Usingwhere;FirstMatch(team);Usingjoinbuffer(BlockNestedLoop)2rowsinset,1warning(0.001sec)  再看看 warnings\n1 2 3 4 5  ***************************1.row***************************Level:NoteCode:1003Message:/* select#1 */select`test`.`team`.`id`AS`id`,`test`.`team`.`status`AS`status`from`test`.`team`semijoin(`test`.`player`)where((`test`.`player`.`team_id`=`test`.`team`.`id`)and(`test`.`player`.`status`=1))1rowinset(0.001sec)  对照着实际运行的语句的 Note, 发现错误的语句缺少了以下这个条件\n ((test.player.team_id = test.team.id)\n 难道就是你! 但是为什么又有\n Field or reference \u0026rsquo;test.team.id\u0026rsquo; of SELECT #2 was resolved in SELECT #1\n 这个问题呢\n总感觉是个Bug\u0026hellip;\u0026hellip;\nTo Be Continued\u0026hellip;\u0026hellip;\n参考资料  Optimizing Subqueries, Derived Tables, and View References with Semijoin Transformations  ","date":"2019-09-21T15:22:58+08:00","permalink":"https://memwey.github.io/p/oh-mysql-in-subquery/","title":"Oh MySQL IN Subquery"},{"content":"最近看到一个有趣的 GNU/Linux 命令, yes\n先 man 一下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  YES(1) BSD General Commands Manual YES(1) NAME yes -- be repetitively affirmative SYNOPSIS yes [expletive] DESCRIPTION yes outputs expletive, or, by default, ``y\u0026#39;\u0026#39;, forever. HISTORY The yes command appeared in 4.0BSD. 4th Berkeley Distribution June 6, 1993 4th Berkeley Distribution (END)   emmmmm, 在 macOS 下并没有 --help 或者 -h 的选项\n试试 Linux 下呢\n1 2 3 4 5 6 7 8 9 10 11  ➜ / yes --help Usage: yes [STRING]... or: yes OPTION Repeatedly output a line with all specified STRING(s), or \u0026#39;y\u0026#39;. --help display this help and exit --version output version information and exit GNU coreutils online help: \u0026lt;https://www.gnu.org/software/coreutils/\u0026gt; Full documentation at: \u0026lt;https://www.gnu.org/software/coreutils/yes\u0026gt; or available locally via: info \u0026#39;(coreutils) yes invocation\u0026#39;   有了\n简单来说这个命令就是可以反复的输出一个字符串, 这个字符串的默认值是 y\n于是有了一些特别的用法, 比如, 注意这里没有 f\n1  yes | rm -r /   还有\n1  yes $(yes yes)   好孩子不要乱试\n当然这个命令的效果其实一般都有替代, 比如加上 -f, -y 什么的, 生成一个大文件也可以用 /dev/urandom 来做\n还可以在冬天做暖手宝, 噗\n最后, 跟我一起在命令行输入\n1  yes \u0026#39;AMD, yes!\u0026#39;   果然人类的本质就是复读机\n","date":"2018-12-14T23:34:15+08:00","permalink":"https://memwey.github.io/p/yes/","title":"Yes"},{"content":"新的博客, 重新开始\n这次尝试着坚持一下吧\n","date":"2018-09-17T23:26:20+08:00","permalink":"https://memwey.github.io/p/%E9%87%8D%E6%96%B0%E5%BC%80%E5%A7%8B/","title":"重新开始"},{"content":"最近研究了一下红黑树的一些性质和思想, 在这里记录一下.\nMap, 或者在 Python 等一些语言中叫做 dictionary 的常用的以键值对形式储存的数据结构一般有两种实现方式, 在 C++ 的 STL 中使用了红黑树的方式, 而在Python中使用了哈希表的方式.\n一般认为采用哈希表的方式查找删除的时间复杂度为 O(1), 而红黑树为 O(logn).\nPython 中的哈希表使用开放寻址法解决冲突.\n对于普通的二叉查找树来说, 查找和插入的时间复杂度在最坏的情况下可能会变成 O(n), 即完全偏向一边的不平衡情况使其成为一个单链表. 如果只是在树的叶子上增加节点而不进行其他的调整, 很容易会使只向下增长的树不平衡. 为了保证 O(logn) 的时间复杂度, 我们需要一种动态的机制, 来调整树的父节点, 乃至于根节点.\n红黑树是一种动态调整的实现方式. 红黑树其实是在 2, 3-树 的基础上实现的. 他们也完全可以等价的转换. 不过 2, 3-树 在程序的实现上比较复杂, 而且查找操作也和二叉搜索树有一些不同, 所以在程序实现中一般使用红黑树.\n2, 3-树 和红黑树他们共同的思想是, 将树的叶子节点上的操作造成的影响, 逐步的传递给父节点, 按照一定的方式对当前子树进行调整. 父节点再传递给它的父节点, 调整更大一些的子树. 最终传递到根节点, 调整整颗树.\n可以这样理解, 在红黑树中, 用红色来标记正在累计调整的节点. 当节点M的两个子节点均被标记, 则M取消子节点的标记, 并标记自己, 使调整向根节点传递.\n","date":"2016-09-13T23:32:27+08:00","permalink":"https://memwey.github.io/p/%E7%BA%A2%E9%BB%91%E6%A0%91%E7%AC%94%E8%AE%B0/","title":"红黑树笔记"},{"content":"公司的服务器都是 CentOS 的, 带的软件都比较旧, 让我这个不更新会死星人很难过啊.\n开玩笑的, 主要问题是 git 版本太旧, clone 的时候会报错, 然后也需要 python3.5. 在 git 上面发现 ius 来解决这个问题.\nius 是一个社区项目, 目的是为 Linux 的企业发行版提供一些更新版本的RPM包.\n1 2  sudo yum install epel-release sudo yum install https://centos6.iuscommunity.org/ius-release.rpm   为了解决一些冲突和共存的问题, 有一些 package 在 ius 中的包名有一些改动\n1 2  sudo yum install git2u sudo yum install python35u   一般的命名规则是\n{name}{major_version}{minor_version}u\n这样就可以在 CentOS 上使用较新的软件啦.\n","date":"2016-08-02T23:30:33+08:00","permalink":"https://memwey.github.io/p/%E5%9C%A8centos%E4%B8%8A%E4%BD%BF%E7%94%A8%E8%BE%83%E6%96%B0%E7%9A%84%E8%BD%AF%E4%BB%B6/","title":"在CentOS上使用较新的软件"},{"content":"最近在跑 DHT 网络拟真的时候涉及到一些 linux 后台运行的一些东西,现在总结一下\n例如\n1  nohup ../src/OverSim -c ChordChurn -u Cmdenv \u0026gt; out.file 2\u0026gt;\u0026amp;1 \u0026amp;   使得 ../src/OverSim -c ChordChurn -u Cmdenv 命令在后台运行,并将 stderr 重定向到 stdout 中然后再将 stdout 重定向到 out.file, 并且在当前终端关闭时仍然运行\n具体说一下吧\nnohup 退出终端的时候,一般在终端中运行的进程会随之关闭,因为此时终端中的子进程会收到 SIGHUP 信号. 而 nohup 使得进程忽略所有 SIGHUP 信号. stdout 默认会重定向到 nohup.out 文件中\n\u0026amp; 在命令中末尾加入 \u0026amp; 符号,使进程在后台运行\n2\u0026gt;\u0026amp;1 放在 \u0026gt; 后面的 \u0026amp;, 表示重定向的目标不是一个文件, 而是一个文件描述符, 文件描述符\n 1 =\u0026gt; stdout 2 =\u0026gt; stderr 0 =\u0026gt; stdin  其他一些命令和操作 Control+z 可以发出 SIGSTOP 信号, 使当前前台进程暂停并放入后台\nfg [job_id]使进程在前台运行\nbg [job_id]使进程在后台运行\njobs 查看后台进程\n","date":"2016-06-15T23:28:24+08:00","permalink":"https://memwey.github.io/p/linux%E5%90%8E%E5%8F%B0%E5%91%BD%E4%BB%A4/","title":"Linux后台命令"},{"content":"其实很早就想弄个博客什么的, 觉得有些东西费了很大的力气解决了, 然后久而久之就忘了\u0026hellip;\u0026hellip;记录下来应该会好一点吧, 这样觉得.\n所以还是写一些东西吧, 嗯嗯\u0026hellip;\u0026hellip;\n其实还是不太喜欢这种不怎么能自己定制的东西\u0026hellip;\u0026hellip;总有一种不知所措的感觉\u0026hellip;\u0026hellip;自己写一个后台倒是没什么问题, 前端的话就实在是无力了, 噗\n对 Markdown 的语法还不那么熟悉, 而且好像 Markdown 有一些不同的解释器, 很麻烦啊\n每次生成的静态网页还要在本地生成了之后看过效果\u0026hellip;\u0026hellip;啊, 真的很麻烦\n","date":"2016-06-14T23:22:32+08:00","permalink":"https://memwey.github.io/p/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/","title":"第一篇博客"}]